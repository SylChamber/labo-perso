# AlmaLinux Incus VM Setup Guide

> Conversation avec Claude 3.7 Sonnet via Zed AI, 2025-06-10
> Info incorrecte. Solution:
> https://discuss.linuxcontainers.org/t/launching-vm-requires-an-agent-config-disk/19257/3
> https://linuxcontainers.org/incus/docs/main/reference/devices_disk/#types-of-disk-devices
>
> ```shell
> # incus profile create vm-agent
> # incus profile device add vm-agent agent disk source=agent:config
> incus init images:almalinux/9 almalinux9 --vm --device root,size=25GiB
> incus config device add almalinux9 agent disk source=agent:config
> incus start almalinux9
> ```

## User

how do i create an AlmaLinux 9.6 VM with incus so that I can test the installation of k3s? the incus software is already installed and configured on my Fedora system.

## Agent

# Creating an AlmaLinux 9.6 VM with Incus for K3s Testing

I'll help you create an AlmaLinux 9.6 VM using Incus to test K3s installation. Let's go through the process step by step.

First, let's check if we can find information about using Incus with AlmaLinux.

**Use Tool: web_search (toolu_01UggvpHXGwiNoLJ577J77WZ)**
```json
{
  "query": "create AlmaLinux 9.6 VM with Incus"
}
```

**Tool Results: toolu_01UggvpHXGwiNoLJ577J77WZ**

{"results":[{"title":"Get AlmaLinux","url":"https://almalinux.org/get-almalinux/","text":"AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"},{"title":"How to create instances","url":"https://linuxcontainers.org/incus/docs/main/howto/instances_create/","text":"\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To create an instance, you can use either the incus init or the incus launch command.\nThe incus init command only creates the instance, while the incus launch command creates and starts it. \n \n Usage ¶ \n Enter the following command to create a container: \n incus launch|init &lt;image_server&gt;:&lt;image_name&gt; &lt;instance_name&gt; [flags]\n \n \n Image Images contain a basic operating system (for example, a Linux distribution) and some Incus-related information.\nImages for various operating systems are available on the built-in remote image servers.\nSee Images for more information. \n Unless the image is available locally, you must specify the name of the image server and the name of the image (for example, images:ubuntu/22.04 for the official 22.04 Ubuntu image). \n \n Instance name Instance names must be unique within an Incus deployment (also within a cluster).\nSee Instance properties for additional requirements. \n \n Flags See incus launch --help or incus init --help for a full list of flags.\nThe most common flags are: \n \n --config to specify a configuration option for the new instance \n --device to override device options for a device provided through a profile, or to specify an initial configuration for the root disk device \n --profile to specify a profile to use for the new instance \n --network or --storage to make the new instance use a specific network or storage pool \n --target to create the instance on a specific cluster member \n --vm to create a virtual machine instead of a container \n \n \n \n \n \n Pass a configuration file ¶ \n Instead of specifying the instance configuration as flags, you can pass it to the command as a YAML file. \n For example, to launch a container with the configuration from config.yaml, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-config &lt; config.yaml\n \n \n \n Examples ¶ \n The following examples use incus launch, but you can use incus init in the same way. \n \n Launch a system container ¶ \n To launch a system container with an Ubuntu 22.04 image from the images server using the instance name ubuntu-container, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-container\n \n \n \n Launch an application container ¶ \n To launch an application (OCI) container, you first need to add an image registry: \n incus remote add oci-docker https://docker.io --protocol=oci\n \n And then can launch a container from one of its images: \n incus launch oci-docker:hello-world --ephemeral --console\n \n \n \n Launch a virtual machine ¶ \n To launch a virtual machine with an Ubuntu 22.04 image from the images server using the instance name ubuntu-vm, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-vm --vm\n \n Or with a bigger disk: \n incus launch images:ubuntu/22.04 ubuntu-vm-big --vm --device root,size=30GiB\n \n \n \n Launch a container with specific configuration options ¶ \n To launch a container and limit its res"},{"title":"Linux Containers - Incus - Introduction","url":"https://linuxcontainers.org/incus/","text":"- Incus\n- Introduction\n\n[Go to Table of Contents](https://linuxcontainers.org/incus/#table-of-contents)\n\n# What is Incus? [¶](https://linuxcontainers.org/incus/\\#what-is-incus)\n\nIncus is a next generation system container and virtual machine manager.\n\nIt provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network.\n\nIncus is image based and provides images for a [wide number of Linux distributions](https://images.linuxcontainers.org). It provides flexibility and scalability for various use cases, with support for different storage backends and network types and the option to install on hardware ranging from an individual laptop or cloud instance to a full server rack.\n\nWhen using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access.\n\nThe Incus project [was created](https://linuxcontainers.org/incus/announcement/) by Aleksa Sarai as a community driven alternative to Canonical's LXD.\n\nToday, it's led and maintained by much of the same people that once created LXD.\n\n## Get started [¶](https://linuxcontainers.org/incus/\\#get-started)\n\nTo get a better idea of what Incus is and what it does, you can [try it online](https://linuxcontainers.org/incus/try-it/)!\n\nThen if you want to run it locally, take a look at our [getting started guide](https://linuxcontainers.org/incus/docs/main/tutorial/first_steps/).\n\n## Containers and virtual machines [¶](https://linuxcontainers.org/incus/\\#containers-and-virtual-machines)\n\nIncus provides support for system containers and virtual machines.\n\nWhen running a system container, Incus simulates a virtual version of a full operating system. To do this, it uses the functionality provided by the kernel running on the host system.\n\nWhen running a virtual machine, Incus uses the hardware of the host system, but the kernel is provided by the virtual machine. Therefore, virtual machines can be used to run, for example, a different operating system.\n\nYou can learn more about the differences between application containers, system containers and virtual machines in [our documentation](https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/).\n\n# Features [¶](https://linuxcontainers.org/incus/\\#features)\n\nSome of the biggest features of Incus are:\n\nCore API\n\n- [Secure by design](https://linuxcontainers.org/incus/docs/main/security) (through unprivileged containers, resource restrictions, authentication, ...)\n- [Intuitive](https://linuxcontainers.org/incus/docs/main/rest-api) (with a simple, clear API and crisp command line experience)\n- [Scalable](https://linuxcontainers.org/incus/docs/main/clustering) (from containers on your laptop to clusters of thousands of compute nodes)\n- [Event based](https://linuxconta"},{"title":"Running Unifi Network Application in an Incus container","url":"https://blog.hetherington.uk/2025/02/running-unifi-network-application-in-an-incus-container/","text":"\n Incus \n \n Setting up the Unifi Network Application and MongoDB using Quadlets in an Alma Linux Incus container. \n \n \n \n \n \n \n \n \n \n \n \n In my previous post I set up an Incus node. \n Setting up Incus with ZFS on NixOS In this guide i setup a NixOS system with ZFS root and bridged networking for use as an Incus container/VM host. The Life &amp; Times of Rocky Hetherington Rocky Hetherington Now i want to start running system containers in Incus. The first container i'm going to run is for the Unifi Network Application and the associated Mongo database. \n The container will have its own IP on the local network with the Unifi and MongoDB services run via Podman. \n I'm using an Alma Linux 9 container as the point releases ship with a modern version of Podman. \n The Incus container \n First log into the Incus node and create the container. \n incus launch images:almalinux/9 unifi --profile bridged\n \n Now let's get the MAC address of the Unifi container \n incus info unifi\n \n Find the MAC address and then reserve an IP address for that MAC on your router. \n Configuring the Unifi container \n Get a shell on the container to begin configuration \n incus shell unifi\n \n Install an editor and Podman \n dnf install nano podman -y\n \n The Unifi Network Application expects a pre-existing Mongo database so let's set that up now. \n MongoDB \n Create the MongoDB initialisation script \n nano -w init-mongo.sh\n \n and copy/paste the following: \n #!/bin/bash\nif which mongosh &gt; /dev/null 2&gt;&amp;1; then\n mongo_init_bin='mongosh'\nelse\n mongo_init_bin='mongo'\nfi\n\"${mongo_init_bin}\" &lt;&lt;EOF\nuse ${MONGO_AUTHSOURCE}\ndb.auth(\"${MONGO_INITDB_ROOT_USERNAME}\", \"${MONGO_INITDB_ROOT_PASSWORD}\")\ndb.createUser({\n user: \"${MONGO_USER}\",\n pwd: \"${MONGO_PASS}\",\n roles: [\n { db: \"${MONGO_DBNAME}\", role: \"dbOwner\" },\n { db: \"${MONGO_DBNAME}_stat\", role: \"dbOwner\" }\n ]\n})\nEOF\n \n This script comes from the LinuxServer page for the Unifi Network Application Docker image. \n Now create a volume for the DB \n podman volume create unifi-db\n \n and launch MongoDB for initialisation \n podman run --rm \\\n-e MONGO_INITDB_ROOT_USERNAME=root \\\n-e MONGO_INITDB_ROOT_PASSWORD=itsasecret \\\n-e MONGO_USER=unifi \\\n-e MONGO_PASS=itsasecret \\\n-e MONGO_DBNAME=unifi \\\n-e MONGO_AUTHSOURCE=admin \\\n-v unifi-db:/data/db:Z \\\n-v /root/init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh:ro \\\ndocker.io/mongo:7.0\n \n Change the passwords as you see fit, but remember them because you'll need them later. \n Wait a minute while the DB is initialised then do a ctrl-c to shut things down and rm init-mongo.sh to remove the initialisation script. \n Quadlet files \n Now that the database is ready it's time to set up the Unifi and Unifi-DB OCI containers. We're going to use Quadlet files for this. \n First change directories \n cd /etc/containers/systemd\n \n We have several files to create. Create the files as named then copy/paste the contents: \n unifi-app.volume \n [Volume]\nVolumeName=unifi-app\n \n unifi-db.volume \n [Volume]\nVolum"},{"title":"Harnessing the Power of Virtual Instances with Incus","url":"https://medium.com/hostspaceng/harnessing-the-power-of-virtual-instances-with-incus-fcb79b4adc83","text":"Virtualization In the evolving landscape of cloud computing, efficient and secure virtualization is paramount. Incus, a modern, open-source system container and virtual machine manager, offers a robust solution for managing virtual instances. Building upon the foundations of LXC and LXD, Incus provides enhanced features and a unified experience for both containers and virtual machines. This article explores the architecture, benefits, and practical implementation of virtual instances using Incus, demonstrating how it can streamline infrastructure management. Understanding Incus Incus is a powerful system container and virtual machine manager that allows users to run any type of workload efficiently while optimizing resources. It scales from a single instance on a machine to a cluster in a full data center rack, making it suitable for both development and production environments. Key Benefits of Incus Unified Management: Incus provides a consistent interface for managing both containers and virtual machines, simplifying operations and reducing the learning curve. Scalability: It supports scaling from a single instance to large clusters, accommodating growing infrastructure needs. Resource Efficiency: By leveraging system containers, Incus ensures efficient resource utilization, leading to cost savings. Flexibility: Supports a wide range of workloads, from development environments to production applications. Open Source: As a community-driven project, Incus is free to use and benefits from continuous contributions and improvements. Setting Up Virtual Instances with Incus Creating and managing virtual instances with Incus involves several steps, from installation to configuration. Here’s a step-by-step guide: 1. Install Incus Begin by installing Incus on your Linux machine. For Ubuntu, you can use the following commands: sudo apt update sudo apt install incus After installation, initialize Incus: incus init 2. Launch a Container To create and start a new container, use the incus launch command: incus launch images:ubuntu/24.04 my-container This command launches a container named my-container using the Ubuntu 22.04 image. 3. Access the Container To execute commands within the container, use the incus exec command: incus exec my-container -- /bin/bash This opens an interactive shell inside the container. 4. Manage Resources Incus allows you to set resource limits to ensure optimal performance. For example, to limit the container to 2 CPUs and 512MB of memory: incus config set my-container limits.cpu 2 incus config set my-container limits.memory 512MB 5. Networking By default, Incus containers have network access. To assign a static IP or configure advanced networking, refer to the Incus networking documentation. Advanced Configurations 1. Storage Management Incus supports various storage backends. To create a storage pool: incus storage create my-pool dir Then, launch a container using this storage pool: incus launch images:ubuntu/24.04 my-container -"}]}


Debug Output:

```json
{
  "results": [
    {
      "title": "Get AlmaLinux",
      "url": "https://almalinux.org/get-almalinux/",
      "text": "AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"
    },
    {
      "title": "How to create instances",
      "url": "https://linuxcontainers.org/incus/docs/main/howto/instances_create/",
      "text": "\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To create an instance, you can use either the incus init or the incus launch command.\nThe incus init command only creates the instance, while the incus launch command creates and starts it. \n \n Usage ¶ \n Enter the following command to create a container: \n incus launch|init &lt;image_server&gt;:&lt;image_name&gt; &lt;instance_name&gt; [flags]\n \n \n Image Images contain a basic operating system (for example, a Linux distribution) and some Incus-related information.\nImages for various operating systems are available on the built-in remote image servers.\nSee Images for more information. \n Unless the image is available locally, you must specify the name of the image server and the name of the image (for example, images:ubuntu/22.04 for the official 22.04 Ubuntu image). \n \n Instance name Instance names must be unique within an Incus deployment (also within a cluster).\nSee Instance properties for additional requirements. \n \n Flags See incus launch --help or incus init --help for a full list of flags.\nThe most common flags are: \n \n --config to specify a configuration option for the new instance \n --device to override device options for a device provided through a profile, or to specify an initial configuration for the root disk device \n --profile to specify a profile to use for the new instance \n --network or --storage to make the new instance use a specific network or storage pool \n --target to create the instance on a specific cluster member \n --vm to create a virtual machine instead of a container \n \n \n \n \n \n Pass a configuration file ¶ \n Instead of specifying the instance configuration as flags, you can pass it to the command as a YAML file. \n For example, to launch a container with the configuration from config.yaml, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-config &lt; config.yaml\n \n \n \n Examples ¶ \n The following examples use incus launch, but you can use incus init in the same way. \n \n Launch a system container ¶ \n To launch a system container with an Ubuntu 22.04 image from the images server using the instance name ubuntu-container, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-container\n \n \n \n Launch an application container ¶ \n To launch an application (OCI) container, you first need to add an image registry: \n incus remote add oci-docker https://docker.io --protocol=oci\n \n And then can launch a container from one of its images: \n incus launch oci-docker:hello-world --ephemeral --console\n \n \n \n Launch a virtual machine ¶ \n To launch a virtual machine with an Ubuntu 22.04 image from the images server using the instance name ubuntu-vm, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-vm --vm\n \n Or with a bigger disk: \n incus launch images:ubuntu/22.04 ubuntu-vm-big --vm --device root,size=30GiB\n \n \n \n Launch a container with specific configuration options ¶ \n To launch a container and limit its res"
    },
    {
      "title": "Linux Containers - Incus - Introduction",
      "url": "https://linuxcontainers.org/incus/",
      "text": "- Incus\n- Introduction\n\n[Go to Table of Contents](https://linuxcontainers.org/incus/#table-of-contents)\n\n# What is Incus? [¶](https://linuxcontainers.org/incus/\\#what-is-incus)\n\nIncus is a next generation system container and virtual machine manager.\n\nIt provides a user experience similar to that of a public cloud. With it, you can easily mix and match both containers and virtual machines, sharing the same underlying storage and network.\n\nIncus is image based and provides images for a [wide number of Linux distributions](https://images.linuxcontainers.org). It provides flexibility and scalability for various use cases, with support for different storage backends and network types and the option to install on hardware ranging from an individual laptop or cloud instance to a full server rack.\n\nWhen using Incus, you can manage your instances (containers and VMs) with a simple command line tool, directly through the REST API or by using third-party tools and integrations. Incus implements a single REST API for both local and remote access.\n\nThe Incus project [was created](https://linuxcontainers.org/incus/announcement/) by Aleksa Sarai as a community driven alternative to Canonical's LXD.\n\nToday, it's led and maintained by much of the same people that once created LXD.\n\n## Get started [¶](https://linuxcontainers.org/incus/\\#get-started)\n\nTo get a better idea of what Incus is and what it does, you can [try it online](https://linuxcontainers.org/incus/try-it/)!\n\nThen if you want to run it locally, take a look at our [getting started guide](https://linuxcontainers.org/incus/docs/main/tutorial/first_steps/).\n\n## Containers and virtual machines [¶](https://linuxcontainers.org/incus/\\#containers-and-virtual-machines)\n\nIncus provides support for system containers and virtual machines.\n\nWhen running a system container, Incus simulates a virtual version of a full operating system. To do this, it uses the functionality provided by the kernel running on the host system.\n\nWhen running a virtual machine, Incus uses the hardware of the host system, but the kernel is provided by the virtual machine. Therefore, virtual machines can be used to run, for example, a different operating system.\n\nYou can learn more about the differences between application containers, system containers and virtual machines in [our documentation](https://linuxcontainers.org/incus/docs/main/explanation/containers_and_vms/).\n\n# Features [¶](https://linuxcontainers.org/incus/\\#features)\n\nSome of the biggest features of Incus are:\n\nCore API\n\n- [Secure by design](https://linuxcontainers.org/incus/docs/main/security) (through unprivileged containers, resource restrictions, authentication, ...)\n- [Intuitive](https://linuxcontainers.org/incus/docs/main/rest-api) (with a simple, clear API and crisp command line experience)\n- [Scalable](https://linuxcontainers.org/incus/docs/main/clustering) (from containers on your laptop to clusters of thousands of compute nodes)\n- [Event based](https://linuxconta"
    },
    {
      "title": "Running Unifi Network Application in an Incus container",
      "url": "https://blog.hetherington.uk/2025/02/running-unifi-network-application-in-an-incus-container/",
      "text": "\n Incus \n \n Setting up the Unifi Network Application and MongoDB using Quadlets in an Alma Linux Incus container. \n \n \n \n \n \n \n \n \n \n \n \n In my previous post I set up an Incus node. \n Setting up Incus with ZFS on NixOS In this guide i setup a NixOS system with ZFS root and bridged networking for use as an Incus container/VM host. The Life &amp; Times of Rocky Hetherington Rocky Hetherington Now i want to start running system containers in Incus. The first container i'm going to run is for the Unifi Network Application and the associated Mongo database. \n The container will have its own IP on the local network with the Unifi and MongoDB services run via Podman. \n I'm using an Alma Linux 9 container as the point releases ship with a modern version of Podman. \n The Incus container \n First log into the Incus node and create the container. \n incus launch images:almalinux/9 unifi --profile bridged\n \n Now let's get the MAC address of the Unifi container \n incus info unifi\n \n Find the MAC address and then reserve an IP address for that MAC on your router. \n Configuring the Unifi container \n Get a shell on the container to begin configuration \n incus shell unifi\n \n Install an editor and Podman \n dnf install nano podman -y\n \n The Unifi Network Application expects a pre-existing Mongo database so let's set that up now. \n MongoDB \n Create the MongoDB initialisation script \n nano -w init-mongo.sh\n \n and copy/paste the following: \n #!/bin/bash\nif which mongosh &gt; /dev/null 2&gt;&amp;1; then\n mongo_init_bin='mongosh'\nelse\n mongo_init_bin='mongo'\nfi\n\"${mongo_init_bin}\" &lt;&lt;EOF\nuse ${MONGO_AUTHSOURCE}\ndb.auth(\"${MONGO_INITDB_ROOT_USERNAME}\", \"${MONGO_INITDB_ROOT_PASSWORD}\")\ndb.createUser({\n user: \"${MONGO_USER}\",\n pwd: \"${MONGO_PASS}\",\n roles: [\n { db: \"${MONGO_DBNAME}\", role: \"dbOwner\" },\n { db: \"${MONGO_DBNAME}_stat\", role: \"dbOwner\" }\n ]\n})\nEOF\n \n This script comes from the LinuxServer page for the Unifi Network Application Docker image. \n Now create a volume for the DB \n podman volume create unifi-db\n \n and launch MongoDB for initialisation \n podman run --rm \\\n-e MONGO_INITDB_ROOT_USERNAME=root \\\n-e MONGO_INITDB_ROOT_PASSWORD=itsasecret \\\n-e MONGO_USER=unifi \\\n-e MONGO_PASS=itsasecret \\\n-e MONGO_DBNAME=unifi \\\n-e MONGO_AUTHSOURCE=admin \\\n-v unifi-db:/data/db:Z \\\n-v /root/init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh:ro \\\ndocker.io/mongo:7.0\n \n Change the passwords as you see fit, but remember them because you'll need them later. \n Wait a minute while the DB is initialised then do a ctrl-c to shut things down and rm init-mongo.sh to remove the initialisation script. \n Quadlet files \n Now that the database is ready it's time to set up the Unifi and Unifi-DB OCI containers. We're going to use Quadlet files for this. \n First change directories \n cd /etc/containers/systemd\n \n We have several files to create. Create the files as named then copy/paste the contents: \n unifi-app.volume \n [Volume]\nVolumeName=unifi-app\n \n unifi-db.volume \n [Volume]\nVolum"
    },
    {
      "title": "Harnessing the Power of Virtual Instances with Incus",
      "url": "https://medium.com/hostspaceng/harnessing-the-power-of-virtual-instances-with-incus-fcb79b4adc83",
      "text": "Virtualization In the evolving landscape of cloud computing, efficient and secure virtualization is paramount. Incus, a modern, open-source system container and virtual machine manager, offers a robust solution for managing virtual instances. Building upon the foundations of LXC and LXD, Incus provides enhanced features and a unified experience for both containers and virtual machines. This article explores the architecture, benefits, and practical implementation of virtual instances using Incus, demonstrating how it can streamline infrastructure management. Understanding Incus Incus is a powerful system container and virtual machine manager that allows users to run any type of workload efficiently while optimizing resources. It scales from a single instance on a machine to a cluster in a full data center rack, making it suitable for both development and production environments. Key Benefits of Incus Unified Management: Incus provides a consistent interface for managing both containers and virtual machines, simplifying operations and reducing the learning curve. Scalability: It supports scaling from a single instance to large clusters, accommodating growing infrastructure needs. Resource Efficiency: By leveraging system containers, Incus ensures efficient resource utilization, leading to cost savings. Flexibility: Supports a wide range of workloads, from development environments to production applications. Open Source: As a community-driven project, Incus is free to use and benefits from continuous contributions and improvements. Setting Up Virtual Instances with Incus Creating and managing virtual instances with Incus involves several steps, from installation to configuration. Here’s a step-by-step guide: 1. Install Incus Begin by installing Incus on your Linux machine. For Ubuntu, you can use the following commands: sudo apt update sudo apt install incus After installation, initialize Incus: incus init 2. Launch a Container To create and start a new container, use the incus launch command: incus launch images:ubuntu/24.04 my-container This command launches a container named my-container using the Ubuntu 22.04 image. 3. Access the Container To execute commands within the container, use the incus exec command: incus exec my-container -- /bin/bash This opens an interactive shell inside the container. 4. Manage Resources Incus allows you to set resource limits to ensure optimal performance. For example, to limit the container to 2 CPUs and 512MB of memory: incus config set my-container limits.cpu 2 incus config set my-container limits.memory 512MB 5. Networking By default, Incus containers have network access. To assign a static IP or configure advanced networking, refer to the Incus networking documentation. Advanced Configurations 1. Storage Management Incus supports various storage backends. To create a storage pool: incus storage create my-pool dir Then, launch a container using this storage pool: incus launch images:ubuntu/24.04 my-container -"
    }
  ]
}
```

## Agent

Now, let's look for specific information about Incus commands for creating VMs:

**Use Tool: web_search (toolu_01KtVrdHuWn6HU3jsu7bB5pW)**
```json
{
  "query": "incus launch vm AlmaLinux 9.6 commands"
}
```

**Tool Results: toolu_01KtVrdHuWn6HU3jsu7bB5pW**

{"results":[{"title":"How to create instances","url":"https://linuxcontainers.org/incus/docs/main/howto/instances_create/","text":"\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To create an instance, you can use either the incus init or the incus launch command.\nThe incus init command only creates the instance, while the incus launch command creates and starts it. \n \n Usage ¶ \n Enter the following command to create a container: \n incus launch|init &lt;image_server&gt;:&lt;image_name&gt; &lt;instance_name&gt; [flags]\n \n \n Image Images contain a basic operating system (for example, a Linux distribution) and some Incus-related information.\nImages for various operating systems are available on the built-in remote image servers.\nSee Images for more information. \n Unless the image is available locally, you must specify the name of the image server and the name of the image (for example, images:ubuntu/22.04 for the official 22.04 Ubuntu image). \n \n Instance name Instance names must be unique within an Incus deployment (also within a cluster).\nSee Instance properties for additional requirements. \n \n Flags See incus launch --help or incus init --help for a full list of flags.\nThe most common flags are: \n \n --config to specify a configuration option for the new instance \n --device to override device options for a device provided through a profile, or to specify an initial configuration for the root disk device \n --profile to specify a profile to use for the new instance \n --network or --storage to make the new instance use a specific network or storage pool \n --target to create the instance on a specific cluster member \n --vm to create a virtual machine instead of a container \n \n \n \n \n \n Pass a configuration file ¶ \n Instead of specifying the instance configuration as flags, you can pass it to the command as a YAML file. \n For example, to launch a container with the configuration from config.yaml, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-config &lt; config.yaml\n \n \n \n Examples ¶ \n The following examples use incus launch, but you can use incus init in the same way. \n \n Launch a system container ¶ \n To launch a system container with an Ubuntu 22.04 image from the images server using the instance name ubuntu-container, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-container\n \n \n \n Launch an application container ¶ \n To launch an application (OCI) container, you first need to add an image registry: \n incus remote add oci-docker https://docker.io --protocol=oci\n \n And then can launch a container from one of its images: \n incus launch oci-docker:hello-world --ephemeral --console\n \n \n \n Launch a virtual machine ¶ \n To launch a virtual machine with an Ubuntu 22.04 image from the images server using the instance name ubuntu-vm, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-vm --vm\n \n Or with a bigger disk: \n incus launch images:ubuntu/22.04 ubuntu-vm-big --vm --device root,size=30GiB\n \n \n \n Launch a container with specific configuration options ¶ \n To launch a container and limit its res"},{"title":"Get AlmaLinux","url":"https://almalinux.org/get-almalinux/","text":"AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"},{"title":"incus.create: Create instances from images | incus-client Commands | Man Pages","url":"https://www.mankier.com/1/incus.create","text":"Create instances from images Synopsis incus create [:] [:][] [flags] Description Description:  Create instances from images Options -c, --config =[] Config key/value to apply to the new instance`` --description =\"\" Instance description`` -d, --device =[] New key/value to apply to a specific device`` --empty [=false] Create an empty instance --environment-file =\"\" Include environment variables from file`` -e, --ephemeral [=false] Ephemeral instance -n, --network =\"\" Network name`` --no-profiles [=false] Create the instance with no profiles applied -p, --profile =[] Profile to apply to the new instance`` -s, --storage =\"\" Storage pool name`` --target =\"\" Cluster member name`` -t, --type =\"\" Instance type`` --vm [=false] Create a virtual machine Options Inherited from Parent Commands --debug [=false] Show all debug messages --force-local [=false] Force using the local unix socket -h, --help [=false] Print help --project =\"\" Override the source project`` -q, --quiet [=false] Don't show progress information --sub-commands [=false] Use with help or --help to view sub-commands -v, --verbose [=false] Show all information messages --version [=false] Print version number Example incus create images:debian/12 u1\nincus create images:debian/12 u1 &lt; config.yaml\nCreate the instance with configuration from config.yaml\nincus launch images:debian/12 v2 --vm -d root,size=50GiB -d root,io.bus=nvme\nCreate and start a virtual machine, overriding the disk size and bus See Also incus(1) History 5-May-2025 Auto generated by spf13/cobra Info May 2025 Auto generated by spf13/cobra"},{"title":"Running Unifi Network Application in an Incus container","url":"https://blog.hetherington.uk/2025/02/running-unifi-network-application-in-an-incus-container/","text":"\n Incus \n \n Setting up the Unifi Network Application and MongoDB using Quadlets in an Alma Linux Incus container. \n \n \n \n \n \n \n \n \n \n \n \n In my previous post I set up an Incus node. \n Setting up Incus with ZFS on NixOS In this guide i setup a NixOS system with ZFS root and bridged networking for use as an Incus container/VM host. The Life &amp; Times of Rocky Hetherington Rocky Hetherington Now i want to start running system containers in Incus. The first container i'm going to run is for the Unifi Network Application and the associated Mongo database. \n The container will have its own IP on the local network with the Unifi and MongoDB services run via Podman. \n I'm using an Alma Linux 9 container as the point releases ship with a modern version of Podman. \n The Incus container \n First log into the Incus node and create the container. \n incus launch images:almalinux/9 unifi --profile bridged\n \n Now let's get the MAC address of the Unifi container \n incus info unifi\n \n Find the MAC address and then reserve an IP address for that MAC on your router. \n Configuring the Unifi container \n Get a shell on the container to begin configuration \n incus shell unifi\n \n Install an editor and Podman \n dnf install nano podman -y\n \n The Unifi Network Application expects a pre-existing Mongo database so let's set that up now. \n MongoDB \n Create the MongoDB initialisation script \n nano -w init-mongo.sh\n \n and copy/paste the following: \n #!/bin/bash\nif which mongosh &gt; /dev/null 2&gt;&amp;1; then\n mongo_init_bin='mongosh'\nelse\n mongo_init_bin='mongo'\nfi\n\"${mongo_init_bin}\" &lt;&lt;EOF\nuse ${MONGO_AUTHSOURCE}\ndb.auth(\"${MONGO_INITDB_ROOT_USERNAME}\", \"${MONGO_INITDB_ROOT_PASSWORD}\")\ndb.createUser({\n user: \"${MONGO_USER}\",\n pwd: \"${MONGO_PASS}\",\n roles: [\n { db: \"${MONGO_DBNAME}\", role: \"dbOwner\" },\n { db: \"${MONGO_DBNAME}_stat\", role: \"dbOwner\" }\n ]\n})\nEOF\n \n This script comes from the LinuxServer page for the Unifi Network Application Docker image. \n Now create a volume for the DB \n podman volume create unifi-db\n \n and launch MongoDB for initialisation \n podman run --rm \\\n-e MONGO_INITDB_ROOT_USERNAME=root \\\n-e MONGO_INITDB_ROOT_PASSWORD=itsasecret \\\n-e MONGO_USER=unifi \\\n-e MONGO_PASS=itsasecret \\\n-e MONGO_DBNAME=unifi \\\n-e MONGO_AUTHSOURCE=admin \\\n-v unifi-db:/data/db:Z \\\n-v /root/init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh:ro \\\ndocker.io/mongo:7.0\n \n Change the passwords as you see fit, but remember them because you'll need them later. \n Wait a minute while the DB is initialised then do a ctrl-c to shut things down and rm init-mongo.sh to remove the initialisation script. \n Quadlet files \n Now that the database is ready it's time to set up the Unifi and Unifi-DB OCI containers. We're going to use Quadlet files for this. \n First change directories \n cd /etc/containers/systemd\n \n We have several files to create. Create the files as named then copy/paste the contents: \n unifi-app.volume \n [Volume]\nVolumeName=unifi-app\n \n unifi-db.volume \n [Volume]\nVolum"},{"title":"Using Incus client to manage our containers and virtual machines","url":"https://www.msandwidi.net/blog/2024/07/22/exploring-incus-client-cli/","text":"\n \n \n \n \n \n Today, we will explore how to use Incus command line client to do basic management of Incus resources. If you need instructions on installing and setting up Incus server for the first time, please refer to my my previous article. So, this article assume that you already have Incus server up and running. \n I will try to keep things simple and brief here again, as Incus documentation page offers much more details on how to manage your workloads using the command line client. \n In the following sections, I will: \n \n Show you how to install Incus client on a Windows machine \n Show you how to run basic commands \n Tell you where to learn more about Incus command line client \n \n 1. Install Incus Command Line Client \n Incus client is the package that allows you to interact with Incus daemon. When you run the command incus --help, you are actually interacting with Incus client. It takes your commands and then forwards them to the daemon. \n Incus client comes preinstalled with Incus server, so you won't need to install it on the machine where you host your containers and virtual machines. \n However, if you want Incus server in a different location and Incus client on your workstation, you will need to install and set up the client. \n To install Incus command line client on a Windows PC, you can use Chocolatey, Winget, or download the native build from Incus client page. \n Installing Incus client using Chocolatey \n \n Make sure you have Chocolatey installed: \n \n \n Checkout Chocolatey installation page if you don't have it on your Windows computer yet. \n \n Install Incus client: \n \n choco install incus \n \n \n For installing Incus client using other methods, please check out this page on Incus documentation page. \n Once Incus client is installed, you need to connect it to the server to manage the resources. \n How to add an Incus server the command line client? \n \n Expose Incus API to the network. On the server, run:: \n \n incus config set core.https_address :8443 \n This commands make Incus API available on your network. \n \n \n On the server, enter the following command: \n \n incus config trust add &lt;CLIENT_NAME&gt; \n Replace client name (CLIENT_NAME) with a descriptive name for your client. For example my_laptop. \n \n \n On the client, add the server by running the command: \n \n incus remote add &lt;SERVER_NAME&gt; \n Replace the server name (SERVER_NAME) with a descriptive name of you Incus server. For example my_server_1. Follow the prompt to add the token given to you by the server. \n Make sure you properly added you Incus server to the client. Run incus remote list to see all the servers your Incus client is connected to. \n 2. Run Basic Commands using Incus Command Line Client \n \n How to create a new container instance? \n \n To create an Incus container called my-container, run the following command: \n incus launch images:ubuntu/22.04 my-container \n By default, the command incus launch creates a system container. If you want a full virtual machine, you"}]}


Debug Output:

```json
{
  "results": [
    {
      "title": "How to create instances",
      "url": "https://linuxcontainers.org/incus/docs/main/howto/instances_create/",
      "text": "\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To create an instance, you can use either the incus init or the incus launch command.\nThe incus init command only creates the instance, while the incus launch command creates and starts it. \n \n Usage ¶ \n Enter the following command to create a container: \n incus launch|init &lt;image_server&gt;:&lt;image_name&gt; &lt;instance_name&gt; [flags]\n \n \n Image Images contain a basic operating system (for example, a Linux distribution) and some Incus-related information.\nImages for various operating systems are available on the built-in remote image servers.\nSee Images for more information. \n Unless the image is available locally, you must specify the name of the image server and the name of the image (for example, images:ubuntu/22.04 for the official 22.04 Ubuntu image). \n \n Instance name Instance names must be unique within an Incus deployment (also within a cluster).\nSee Instance properties for additional requirements. \n \n Flags See incus launch --help or incus init --help for a full list of flags.\nThe most common flags are: \n \n --config to specify a configuration option for the new instance \n --device to override device options for a device provided through a profile, or to specify an initial configuration for the root disk device \n --profile to specify a profile to use for the new instance \n --network or --storage to make the new instance use a specific network or storage pool \n --target to create the instance on a specific cluster member \n --vm to create a virtual machine instead of a container \n \n \n \n \n \n Pass a configuration file ¶ \n Instead of specifying the instance configuration as flags, you can pass it to the command as a YAML file. \n For example, to launch a container with the configuration from config.yaml, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-config &lt; config.yaml\n \n \n \n Examples ¶ \n The following examples use incus launch, but you can use incus init in the same way. \n \n Launch a system container ¶ \n To launch a system container with an Ubuntu 22.04 image from the images server using the instance name ubuntu-container, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-container\n \n \n \n Launch an application container ¶ \n To launch an application (OCI) container, you first need to add an image registry: \n incus remote add oci-docker https://docker.io --protocol=oci\n \n And then can launch a container from one of its images: \n incus launch oci-docker:hello-world --ephemeral --console\n \n \n \n Launch a virtual machine ¶ \n To launch a virtual machine with an Ubuntu 22.04 image from the images server using the instance name ubuntu-vm, enter the following command: \n incus launch images:ubuntu/22.04 ubuntu-vm --vm\n \n Or with a bigger disk: \n incus launch images:ubuntu/22.04 ubuntu-vm-big --vm --device root,size=30GiB\n \n \n \n Launch a container with specific configuration options ¶ \n To launch a container and limit its res"
    },
    {
      "title": "Get AlmaLinux",
      "url": "https://almalinux.org/get-almalinux/",
      "text": "AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"
    },
    {
      "title": "incus.create: Create instances from images | incus-client Commands | Man Pages",
      "url": "https://www.mankier.com/1/incus.create",
      "text": "Create instances from images Synopsis incus create [:] [:][] [flags] Description Description:  Create instances from images Options -c, --config =[] Config key/value to apply to the new instance`` --description =\"\" Instance description`` -d, --device =[] New key/value to apply to a specific device`` --empty [=false] Create an empty instance --environment-file =\"\" Include environment variables from file`` -e, --ephemeral [=false] Ephemeral instance -n, --network =\"\" Network name`` --no-profiles [=false] Create the instance with no profiles applied -p, --profile =[] Profile to apply to the new instance`` -s, --storage =\"\" Storage pool name`` --target =\"\" Cluster member name`` -t, --type =\"\" Instance type`` --vm [=false] Create a virtual machine Options Inherited from Parent Commands --debug [=false] Show all debug messages --force-local [=false] Force using the local unix socket -h, --help [=false] Print help --project =\"\" Override the source project`` -q, --quiet [=false] Don't show progress information --sub-commands [=false] Use with help or --help to view sub-commands -v, --verbose [=false] Show all information messages --version [=false] Print version number Example incus create images:debian/12 u1\nincus create images:debian/12 u1 &lt; config.yaml\nCreate the instance with configuration from config.yaml\nincus launch images:debian/12 v2 --vm -d root,size=50GiB -d root,io.bus=nvme\nCreate and start a virtual machine, overriding the disk size and bus See Also incus(1) History 5-May-2025 Auto generated by spf13/cobra Info May 2025 Auto generated by spf13/cobra"
    },
    {
      "title": "Running Unifi Network Application in an Incus container",
      "url": "https://blog.hetherington.uk/2025/02/running-unifi-network-application-in-an-incus-container/",
      "text": "\n Incus \n \n Setting up the Unifi Network Application and MongoDB using Quadlets in an Alma Linux Incus container. \n \n \n \n \n \n \n \n \n \n \n \n In my previous post I set up an Incus node. \n Setting up Incus with ZFS on NixOS In this guide i setup a NixOS system with ZFS root and bridged networking for use as an Incus container/VM host. The Life &amp; Times of Rocky Hetherington Rocky Hetherington Now i want to start running system containers in Incus. The first container i'm going to run is for the Unifi Network Application and the associated Mongo database. \n The container will have its own IP on the local network with the Unifi and MongoDB services run via Podman. \n I'm using an Alma Linux 9 container as the point releases ship with a modern version of Podman. \n The Incus container \n First log into the Incus node and create the container. \n incus launch images:almalinux/9 unifi --profile bridged\n \n Now let's get the MAC address of the Unifi container \n incus info unifi\n \n Find the MAC address and then reserve an IP address for that MAC on your router. \n Configuring the Unifi container \n Get a shell on the container to begin configuration \n incus shell unifi\n \n Install an editor and Podman \n dnf install nano podman -y\n \n The Unifi Network Application expects a pre-existing Mongo database so let's set that up now. \n MongoDB \n Create the MongoDB initialisation script \n nano -w init-mongo.sh\n \n and copy/paste the following: \n #!/bin/bash\nif which mongosh &gt; /dev/null 2&gt;&amp;1; then\n mongo_init_bin='mongosh'\nelse\n mongo_init_bin='mongo'\nfi\n\"${mongo_init_bin}\" &lt;&lt;EOF\nuse ${MONGO_AUTHSOURCE}\ndb.auth(\"${MONGO_INITDB_ROOT_USERNAME}\", \"${MONGO_INITDB_ROOT_PASSWORD}\")\ndb.createUser({\n user: \"${MONGO_USER}\",\n pwd: \"${MONGO_PASS}\",\n roles: [\n { db: \"${MONGO_DBNAME}\", role: \"dbOwner\" },\n { db: \"${MONGO_DBNAME}_stat\", role: \"dbOwner\" }\n ]\n})\nEOF\n \n This script comes from the LinuxServer page for the Unifi Network Application Docker image. \n Now create a volume for the DB \n podman volume create unifi-db\n \n and launch MongoDB for initialisation \n podman run --rm \\\n-e MONGO_INITDB_ROOT_USERNAME=root \\\n-e MONGO_INITDB_ROOT_PASSWORD=itsasecret \\\n-e MONGO_USER=unifi \\\n-e MONGO_PASS=itsasecret \\\n-e MONGO_DBNAME=unifi \\\n-e MONGO_AUTHSOURCE=admin \\\n-v unifi-db:/data/db:Z \\\n-v /root/init-mongo.sh:/docker-entrypoint-initdb.d/init-mongo.sh:ro \\\ndocker.io/mongo:7.0\n \n Change the passwords as you see fit, but remember them because you'll need them later. \n Wait a minute while the DB is initialised then do a ctrl-c to shut things down and rm init-mongo.sh to remove the initialisation script. \n Quadlet files \n Now that the database is ready it's time to set up the Unifi and Unifi-DB OCI containers. We're going to use Quadlet files for this. \n First change directories \n cd /etc/containers/systemd\n \n We have several files to create. Create the files as named then copy/paste the contents: \n unifi-app.volume \n [Volume]\nVolumeName=unifi-app\n \n unifi-db.volume \n [Volume]\nVolum"
    },
    {
      "title": "Using Incus client to manage our containers and virtual machines",
      "url": "https://www.msandwidi.net/blog/2024/07/22/exploring-incus-client-cli/",
      "text": "\n \n \n \n \n \n Today, we will explore how to use Incus command line client to do basic management of Incus resources. If you need instructions on installing and setting up Incus server for the first time, please refer to my my previous article. So, this article assume that you already have Incus server up and running. \n I will try to keep things simple and brief here again, as Incus documentation page offers much more details on how to manage your workloads using the command line client. \n In the following sections, I will: \n \n Show you how to install Incus client on a Windows machine \n Show you how to run basic commands \n Tell you where to learn more about Incus command line client \n \n 1. Install Incus Command Line Client \n Incus client is the package that allows you to interact with Incus daemon. When you run the command incus --help, you are actually interacting with Incus client. It takes your commands and then forwards them to the daemon. \n Incus client comes preinstalled with Incus server, so you won't need to install it on the machine where you host your containers and virtual machines. \n However, if you want Incus server in a different location and Incus client on your workstation, you will need to install and set up the client. \n To install Incus command line client on a Windows PC, you can use Chocolatey, Winget, or download the native build from Incus client page. \n Installing Incus client using Chocolatey \n \n Make sure you have Chocolatey installed: \n \n \n Checkout Chocolatey installation page if you don't have it on your Windows computer yet. \n \n Install Incus client: \n \n choco install incus \n \n \n For installing Incus client using other methods, please check out this page on Incus documentation page. \n Once Incus client is installed, you need to connect it to the server to manage the resources. \n How to add an Incus server the command line client? \n \n Expose Incus API to the network. On the server, run:: \n \n incus config set core.https_address :8443 \n This commands make Incus API available on your network. \n \n \n On the server, enter the following command: \n \n incus config trust add &lt;CLIENT_NAME&gt; \n Replace client name (CLIENT_NAME) with a descriptive name for your client. For example my_laptop. \n \n \n On the client, add the server by running the command: \n \n incus remote add &lt;SERVER_NAME&gt; \n Replace the server name (SERVER_NAME) with a descriptive name of you Incus server. For example my_server_1. Follow the prompt to add the token given to you by the server. \n Make sure you properly added you Incus server to the client. Run incus remote list to see all the servers your Incus client is connected to. \n 2. Run Basic Commands using Incus Command Line Client \n \n How to create a new container instance? \n \n To create an Incus container called my-container, run the following command: \n incus launch images:ubuntu/22.04 my-container \n By default, the command incus launch creates a system container. If you want a full virtual machine, you"
    }
  ]
}
```

## Agent

Now let's check for information about AlmaLinux 9.6 availability in Incus:

**Use Tool: web_search (toolu_01JNycRn5PyE8qMDrKd9UNCq)**
```json
{
  "query": "check available images incus almalinux 9.6"
}
```

**Tool Results: toolu_01JNycRn5PyE8qMDrKd9UNCq**

{"results":[{"title":"Index of /images/almalinux/","url":"https://images.linuxcontainers.org/images/almalinux/","text":"# Index of /images/almalinux/\n\n* * *\n\n```\n../\n8/ 01-Jul-2023 15:40 -\n9/ 01-Jul-2023 15:40 -\n\n```\n\n* * *"},{"title":"General Availability of AlmaLinux 9.6 Stable!","url":"https://almalinux.org/blog/2025-05-20-almalinux_96_release/","text":"## AlmaLinux 9.6 Stable Now Available\n\nHello Community! The AlmaLinux OS Foundation is announcing the general availability of [AlmaLinux OS 9.6](https://mirrors.almalinux.org/isos.html) codenamed “Sage Margay”!\n\nInstallation ISOs are available on the mirrors now for all 4 architectures:\n\n- [Intel/AMD (x86\\_64)](https://mirrors.almalinux.org/isos/x86_64/9.6.html)\n- [ARM64 (aarch64)](https://mirrors.almalinux.org/isos/aarch64/9.6.html)\n- [IBM PowerPC (ppc64le)](https://mirrors.almalinux.org/isos/ppc64le/9.6.html)\n- [IBM Z (s390x)](https://mirrors.almalinux.org/isos/s390x/9.6.html)\n\nTorrents are available as well at:\n\n- [Intel/AMD (x86\\_64)](https://repo.almalinux.org/almalinux/9.6/isos/x86_64/AlmaLinux-9.6-x86_64.torrent)\n- [ARM64 (aarch64)](https://repo.almalinux.org/almalinux/9.6/isos/aarch64/AlmaLinux-9.6-aarch64.torrent)\n- [IBM PowerPC (ppc64le)](https://repo.almalinux.org/almalinux/9.6/isos/ppc64le/AlmaLinux-9.6-ppc64le.torrent)\n- [IBM Z (s390x)](https://repo.almalinux.org/almalinux/9.6/isos/s390x/AlmaLinux-9.6-s390x.torrent)\n\n## ISOs, Live Images, Cloud and Containers\n\nAlmaLinux also offers a variety of Cloud, Container and Live Images. The builds for these get kicked off as soon as the public repository is ready.\n\n**The following images are expected to be available shortly.**\n\n- [Container](https://wiki.almalinux.org/containers/) images including Platform and UBIs alternatives. We provide a wide variety of containers for your use.\n- [LXC/LXD](https://images.linuxcontainers.org/images/almalinux/)\n- [Live Media](https://wiki.almalinux.org/LiveMedia.html) for GNOME, GNOME-mini, KDE, XFCE, MATE and more.\n- Cloud Images\n - [AWS](https://wiki.almalinux.org/cloud/AWS.html) for x86\\_64 and AArch64 EC2 Instances\n - [Azure](https://wiki.almalinux.org/cloud/Azure.html) for x86\\_64 and AArch64 VMs\n - [Google Cloud](https://wiki.almalinux.org/cloud/Google.html)\n - [Generic Cloud/Cloud-init](https://wiki.almalinux.org/cloud/Generic-cloud-on-local.html) for all 4 architectures\n - [OpenNebula](https://wiki.almalinux.org/cloud/OpenNebula.html) for x86\\_64 and AArch64 architectures\n - [Oracle Cloud Infrastructure](https://wiki.almalinux.org/cloud/OCI.html) for x86\\_64 and AArch64 Instances\n- [Vagrant Boxes](https://app.vagrantup.com/almalinux):\n - Libvirt\n - VirtualBox\n - Hyper-V\n - VMWare for x86\\_64 and AArch64\n - Parallels for AArch64\n- [Raspberry Pi](https://wiki.almalinux.org/documentation/raspberry-pi.html)\n- [Windows Subsystem for Linux](https://wiki.almalinux.org/documentation/wsl.html) for x86\\_64 and AArch64\n\n## Release Notes and More Information\n\nAlmaLinux 9.6 Stable continues to enhance performance, development tools, and security. Updated module streams improve support for web applications, while new compiler versions bring optimizations for better performance. The release also delivers updates to elfutils, Valgrind, SystemTap, and PCP improving system debugging and performance monitoring. Networking improvements come with new versions of Network"},{"title":"Get AlmaLinux","url":"https://almalinux.org/get-almalinux/","text":"AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"},{"title":"Index of /almalinux/9/cloud/x86_64/images/","url":"https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/","text":"# Mirror provided by [KnownHost](https://www.knownhost.com) Index of /almalinux/9/cloud/x86\\_64/images/\n\n# Index of /almalinux/9/cloud/x86\\_64/images/\n\n* * *\n\n```\n../\nAlmaLinux-9-GenericCloud-9.4-20240507.x86_64.qcow2 07-May-2024 18:34 602603520\nAlmaLinux-9-GenericCloud-latest.x86_64.qcow2 07-May-2024 18:34 602603520\nAlmaLinux-9-OpenNebula-9.4-20240507.x86_64.qcow2 07-May-2024 18:34 913244160\nAlmaLinux-9-OpenNebula-latest.x86_64.qcow2 07-May-2024 18:34 913244160\nCHECKSUM 08-May-2024 06:32 452\nCHECKSUM.asc 08-May-2024 06:36 833\n\n```\n\n* * *\n\n![Daily bandwidth graph](https://atl.mirrors.knownhost.com/rrd-bandwidth/enp1s0_day.png)\n\n![Monthly bandwidth graph](https://atl.mirrors.knownhost.com/rrd-bandwidth/enp1s0_month.png)"},{"title":"AlmaLinux Cloud, Container and Live Media Images Updates","url":"https://almalinux.org/ja/blog/2025-06-05-almalinux-96-images-updates/","text":"Hey Community! Bringing the latest AlmaLinux OS 9.6 images updates.\n\n## Cloud and Container Images\n\nAlmaLinux provides a variety of cloud images that are now updated to the AlmaLinux OS 9.6 version including:\n\n- [Amazon Web Services (AWS)](https://wiki.almalinux.org/cloud/AWS.html)\n- [Generic Cloud](https://wiki.almalinux.org/cloud/Generic-cloud.html)\n- [OpenNebula](https://wiki.almalinux.org/cloud/OpenNebula.html)\n- [Microsoft Azure](https://wiki.almalinux.org/cloud/Azure.html)\n- [Oracle Cloud Infrastructure](https://cloudmarketplace.oracle.com/marketplace/en_US/listing/127985411/)\n\nYou can find more details in the [Cloud Images Changelog](https://wiki.almalinux.org/cloud/cloud-changelog.html) at the AlmaLinux Wiki.\n\nContainer images are updated to the 9.6 version as well:\n\n- You can get official [Docker](https://hub.docker.com/_/almalinux) images and [OCI Images from Quay.io](https://quay.io/organization/almalinuxorg). You can also get container images from [GitHub packages](https://github.com/orgs/AlmaLinux/packages)\n- [bootc images](https://quay.io/repository/almalinuxorg/almalinux-bootc?tab=tags)\n\nIf you want to contribute to Cloud or Container images, or have any questions, please check with the [Cloud SIG](https://wiki.almalinux.org/sigs/Cloud.html).\n\nAll Live Media images that include GNOME, GNOME-Mini, KDE, XFCE and MATE are updated to [AlmaLinux OS 9.6](https://repo.almalinux.org/almalinux/9.6/live/) version.\n\n## More options\n\nThe following options have also been updated to the latest versions:\n\n- Incus and LXC to [AlmaLinux 9.6](https://images.linuxcontainers.org/images/almalinux/9/).\n- Raspberry Pi images to [AlmaLinux 9.6](https://repo.almalinux.org/almalinux/9.6/raspberrypi/images/).\n- Vagrant Boxes to [AlmaLinux 9.6](https://portal.cloud.hashicorp.com/vagrant/discover/almalinux/9)\n- Windows Subsystem for Linux (WSL) to [AlmaLinux 9.6](https://wiki.almalinux.org/documentation/wsl.html).\n\nYou can check the [AlmaLinux website](https://almalinux.org/get-almalinux/) for a full list of images AlmaLinux provides.\n\n## How to Help and Сontribute\n\nAll your contributions, feedback and bug reports are greatly appreciated and help us improve AlmaLinux for everyone!\n\nFill in, track and discuss bugs on [bugs.almalinux.org](https://bugs.almalinux.org/).\n\nIf you would like to help support AlmaLinux, there are many ways to contribute - [testing](https://wiki.almalinux.org/Contribute-to-Testing.html), quality assurance, [documentation](https://wiki.almalinux.org/Contribute-to-Documentation.html), and [more](https://wiki.almalinux.org/Contribute.html). We’d be thrilled to have you join us!\n\nIf you need any help or you’d like to discuss anything, check the [Help and Support](https://wiki.almalinux.org/Help-and-Support.html) wiki page, and join us on the [AlmaLinux Community Chat](https://chat.almalinux.org). Engage in the AlmaLinux community on [Reddit](https://reddit.com/r/almalinux), or follow us on [Mastodon](https://fosstodon.org/@almalinux), and ["}]}


Debug Output:

```json
{
  "results": [
    {
      "title": "Index of /images/almalinux/",
      "url": "https://images.linuxcontainers.org/images/almalinux/",
      "text": "# Index of /images/almalinux/\n\n* * *\n\n```\n../\n8/ 01-Jul-2023 15:40 -\n9/ 01-Jul-2023 15:40 -\n\n```\n\n* * *"
    },
    {
      "title": "General Availability of AlmaLinux 9.6 Stable!",
      "url": "https://almalinux.org/blog/2025-05-20-almalinux_96_release/",
      "text": "## AlmaLinux 9.6 Stable Now Available\n\nHello Community! The AlmaLinux OS Foundation is announcing the general availability of [AlmaLinux OS 9.6](https://mirrors.almalinux.org/isos.html) codenamed “Sage Margay”!\n\nInstallation ISOs are available on the mirrors now for all 4 architectures:\n\n- [Intel/AMD (x86\\_64)](https://mirrors.almalinux.org/isos/x86_64/9.6.html)\n- [ARM64 (aarch64)](https://mirrors.almalinux.org/isos/aarch64/9.6.html)\n- [IBM PowerPC (ppc64le)](https://mirrors.almalinux.org/isos/ppc64le/9.6.html)\n- [IBM Z (s390x)](https://mirrors.almalinux.org/isos/s390x/9.6.html)\n\nTorrents are available as well at:\n\n- [Intel/AMD (x86\\_64)](https://repo.almalinux.org/almalinux/9.6/isos/x86_64/AlmaLinux-9.6-x86_64.torrent)\n- [ARM64 (aarch64)](https://repo.almalinux.org/almalinux/9.6/isos/aarch64/AlmaLinux-9.6-aarch64.torrent)\n- [IBM PowerPC (ppc64le)](https://repo.almalinux.org/almalinux/9.6/isos/ppc64le/AlmaLinux-9.6-ppc64le.torrent)\n- [IBM Z (s390x)](https://repo.almalinux.org/almalinux/9.6/isos/s390x/AlmaLinux-9.6-s390x.torrent)\n\n## ISOs, Live Images, Cloud and Containers\n\nAlmaLinux also offers a variety of Cloud, Container and Live Images. The builds for these get kicked off as soon as the public repository is ready.\n\n**The following images are expected to be available shortly.**\n\n- [Container](https://wiki.almalinux.org/containers/) images including Platform and UBIs alternatives. We provide a wide variety of containers for your use.\n- [LXC/LXD](https://images.linuxcontainers.org/images/almalinux/)\n- [Live Media](https://wiki.almalinux.org/LiveMedia.html) for GNOME, GNOME-mini, KDE, XFCE, MATE and more.\n- Cloud Images\n - [AWS](https://wiki.almalinux.org/cloud/AWS.html) for x86\\_64 and AArch64 EC2 Instances\n - [Azure](https://wiki.almalinux.org/cloud/Azure.html) for x86\\_64 and AArch64 VMs\n - [Google Cloud](https://wiki.almalinux.org/cloud/Google.html)\n - [Generic Cloud/Cloud-init](https://wiki.almalinux.org/cloud/Generic-cloud-on-local.html) for all 4 architectures\n - [OpenNebula](https://wiki.almalinux.org/cloud/OpenNebula.html) for x86\\_64 and AArch64 architectures\n - [Oracle Cloud Infrastructure](https://wiki.almalinux.org/cloud/OCI.html) for x86\\_64 and AArch64 Instances\n- [Vagrant Boxes](https://app.vagrantup.com/almalinux):\n - Libvirt\n - VirtualBox\n - Hyper-V\n - VMWare for x86\\_64 and AArch64\n - Parallels for AArch64\n- [Raspberry Pi](https://wiki.almalinux.org/documentation/raspberry-pi.html)\n- [Windows Subsystem for Linux](https://wiki.almalinux.org/documentation/wsl.html) for x86\\_64 and AArch64\n\n## Release Notes and More Information\n\nAlmaLinux 9.6 Stable continues to enhance performance, development tools, and security. Updated module streams improve support for web applications, while new compiler versions bring optimizations for better performance. The release also delivers updates to elfutils, Valgrind, SystemTap, and PCP improving system debugging and performance monitoring. Networking improvements come with new versions of Network"
    },
    {
      "title": "Get AlmaLinux",
      "url": "https://almalinux.org/get-almalinux/",
      "text": "AlmaLinux 9.4 and AlmaLinux 8.10 are both stable! Checkout [AlmaLinux OS 9.4 Stable](https://almalinux.org/blog/2024-05-06-announcing-94-stable/) and [AlmaLinux OS 8.10 Stable](https://almalinux.org/blog/2024-05-28-announcing-810-stable/) for more information!\n\n# Get AlmaLinux OS\n\n## **Intel/AMD (x86\\_64)**\n\n## **ARM64 (AArch64)**\n\n## **IBM Z (s390x)**\n\n## **Power LE (ppc64le)**\n\n[![ISO Image](https://almalinux.org/images/iso-icon-F.svg)](https://almalinux.org/get-almalinux/#ISO_Images)[![Cloud Image](https://almalinux.org/images/cloud-icon-F.svg)](https://almalinux.org/get-almalinux/#Cloud_Images)[![Container Image](https://almalinux.org/images/container-icon-F.svg)](https://almalinux.org/get-almalinux/#Container_Images)[![Live Media Image](https://almalinux.org/images/live-icon-F.svg)](https://almalinux.org/get-almalinux/#Live_Media)[![Vagrant Image](https://almalinux.org/images/vagrant-icon-F1.svg)](https://almalinux.org/get-almalinux/#Vagrant_Boxes)[![LXD-LXC Image](https://almalinux.org/images/lxd-lcx-icon-F.svg)](https://almalinux.org/get-almalinux/#LXD-LXC)[![WSL Image](https://almalinux.org/images/wsl-icon-F.svg)](https://almalinux.org/get-almalinux/#WSL)\n\n## ISO Images\n\nEach supported architecture has various installation images available:\n\n**DVD**\n\\- effortless installation with all packages included, no additional repositories required.\n\n**Boot**\n\\- a single network installation CD image that downloads packages over the Internet.\n\n**Minimal**\n\\- a minimal self-containing DVD image that makes possible offline installation.\n\n### AlmaLinux OS 9\n\n#### [AlmaLinux OS 9.4 DVD ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Boot ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 9.4 Minimal ISO](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n**[Download ISOs from mirrors](https://mirrors.almalinux.org/isos/x86_64/9.4.html)** **[Download .torrent file](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/AlmaLinux-9.4-x86_64.torrent)** **[Download CHECKSUM](https://repo.almalinux.org/almalinux/9.4/isos/x86_64/CHECKSUM)**\n\n### AlmaLinux OS 8\n\n#### [AlmaLinux OS 8.10 DVD ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-dvd.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Boot ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-boot.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-A.svg)\n\nSHA-256:\n\n#### [AlmaLinux OS 8.10 Minimal ISO](https://repo.almalinux.org/almalinux/8.10/isos/x86_64/AlmaLinux-8.10-x86_64-minimal.iso)\n\n![](https://almalinux.org/images/alma-fingerprint-"
    },
    {
      "title": "Index of /almalinux/9/cloud/x86_64/images/",
      "url": "https://repo.almalinux.org/almalinux/9/cloud/x86_64/images/",
      "text": "# Mirror provided by [KnownHost](https://www.knownhost.com) Index of /almalinux/9/cloud/x86\\_64/images/\n\n# Index of /almalinux/9/cloud/x86\\_64/images/\n\n* * *\n\n```\n../\nAlmaLinux-9-GenericCloud-9.4-20240507.x86_64.qcow2 07-May-2024 18:34 602603520\nAlmaLinux-9-GenericCloud-latest.x86_64.qcow2 07-May-2024 18:34 602603520\nAlmaLinux-9-OpenNebula-9.4-20240507.x86_64.qcow2 07-May-2024 18:34 913244160\nAlmaLinux-9-OpenNebula-latest.x86_64.qcow2 07-May-2024 18:34 913244160\nCHECKSUM 08-May-2024 06:32 452\nCHECKSUM.asc 08-May-2024 06:36 833\n\n```\n\n* * *\n\n![Daily bandwidth graph](https://atl.mirrors.knownhost.com/rrd-bandwidth/enp1s0_day.png)\n\n![Monthly bandwidth graph](https://atl.mirrors.knownhost.com/rrd-bandwidth/enp1s0_month.png)"
    },
    {
      "title": "AlmaLinux Cloud, Container and Live Media Images Updates",
      "url": "https://almalinux.org/ja/blog/2025-06-05-almalinux-96-images-updates/",
      "text": "Hey Community! Bringing the latest AlmaLinux OS 9.6 images updates.\n\n## Cloud and Container Images\n\nAlmaLinux provides a variety of cloud images that are now updated to the AlmaLinux OS 9.6 version including:\n\n- [Amazon Web Services (AWS)](https://wiki.almalinux.org/cloud/AWS.html)\n- [Generic Cloud](https://wiki.almalinux.org/cloud/Generic-cloud.html)\n- [OpenNebula](https://wiki.almalinux.org/cloud/OpenNebula.html)\n- [Microsoft Azure](https://wiki.almalinux.org/cloud/Azure.html)\n- [Oracle Cloud Infrastructure](https://cloudmarketplace.oracle.com/marketplace/en_US/listing/127985411/)\n\nYou can find more details in the [Cloud Images Changelog](https://wiki.almalinux.org/cloud/cloud-changelog.html) at the AlmaLinux Wiki.\n\nContainer images are updated to the 9.6 version as well:\n\n- You can get official [Docker](https://hub.docker.com/_/almalinux) images and [OCI Images from Quay.io](https://quay.io/organization/almalinuxorg). You can also get container images from [GitHub packages](https://github.com/orgs/AlmaLinux/packages)\n- [bootc images](https://quay.io/repository/almalinuxorg/almalinux-bootc?tab=tags)\n\nIf you want to contribute to Cloud or Container images, or have any questions, please check with the [Cloud SIG](https://wiki.almalinux.org/sigs/Cloud.html).\n\nAll Live Media images that include GNOME, GNOME-Mini, KDE, XFCE and MATE are updated to [AlmaLinux OS 9.6](https://repo.almalinux.org/almalinux/9.6/live/) version.\n\n## More options\n\nThe following options have also been updated to the latest versions:\n\n- Incus and LXC to [AlmaLinux 9.6](https://images.linuxcontainers.org/images/almalinux/9/).\n- Raspberry Pi images to [AlmaLinux 9.6](https://repo.almalinux.org/almalinux/9.6/raspberrypi/images/).\n- Vagrant Boxes to [AlmaLinux 9.6](https://portal.cloud.hashicorp.com/vagrant/discover/almalinux/9)\n- Windows Subsystem for Linux (WSL) to [AlmaLinux 9.6](https://wiki.almalinux.org/documentation/wsl.html).\n\nYou can check the [AlmaLinux website](https://almalinux.org/get-almalinux/) for a full list of images AlmaLinux provides.\n\n## How to Help and Сontribute\n\nAll your contributions, feedback and bug reports are greatly appreciated and help us improve AlmaLinux for everyone!\n\nFill in, track and discuss bugs on [bugs.almalinux.org](https://bugs.almalinux.org/).\n\nIf you would like to help support AlmaLinux, there are many ways to contribute - [testing](https://wiki.almalinux.org/Contribute-to-Testing.html), quality assurance, [documentation](https://wiki.almalinux.org/Contribute-to-Documentation.html), and [more](https://wiki.almalinux.org/Contribute.html). We’d be thrilled to have you join us!\n\nIf you need any help or you’d like to discuss anything, check the [Help and Support](https://wiki.almalinux.org/Help-and-Support.html) wiki page, and join us on the [AlmaLinux Community Chat](https://chat.almalinux.org). Engage in the AlmaLinux community on [Reddit](https://reddit.com/r/almalinux), or follow us on [Mastodon](https://fosstodon.org/@almalinux), and ["
    }
  ]
}
```

## Agent

Let's check specifically how to list available Incus images:

**Use Tool: web_search (toolu_015xd4wocErS27C8xaQ9xd1d)**
```json
{
  "query": "incus list available images command"
}
```

**Tool Results: toolu_015xd4wocErS27C8xaQ9xd1d**

{"results":[{"title":"How to find Virtual Machine (VM) image with Incus / LXD","url":"https://www.cyberciti.biz/faq/lxd-incus-finding-virtual-machine-vm-linux-command/","text":"\n B oth Incus and LXD, are the powerful system container and virtual machine manager, simplifies many aspects of virtualization. But how do you find the right VM (virtual machine) only image to get you started? Incus and LXD does provides a robust search functionality that makes discovering and selecting images easy for Linux developers and sysadmins. \n \n Finding Your Virtual Machine (VM) image using list option \n \n The main command is to all list images. \n There are two types of images: a “container” and a “virtual machine.” \n To list container only images type: \n To list VM ( virtual machine) only images type: \n \n How to find Virtual Machine (VM) image with (LXD) \n The commands are same but the admin utility name is lxc command. In other words, type the following two commands to list different types of images: \n \n Show all images run: \n See all container images type: \n LXD see all virtual machine (VM) images run: \n \n \n \n Tutorial details \n Difficulty level Easy Root privileges No Requirements Linux terminal Category LXD OS compatibility AlmaLinux • Alpine • Amazon Linux • Arch • CentOS • Debian • Fedora • Linux • Mint • Mint • openSUSE • Pop!_OS • RHEL • Rocky • Slackware • Stream • SUSE • Ubuntu Est. reading time 3 minutes \n How to list only container images with operating system name \n When you type the following command, you get all images regardless of their architecture or operating system (OS) \n $ incus image list images: \n # LXD (LXC) # \n $ lxc image list images: \nOutputs: \n +------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| ALIAS | FINGERPRINT | PUBLIC | DESCRIPTION | ARCHITECTURE | TYPE | SIZE | UPLOAD DATE |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 ( 3 more) | 0c4bc559d302 | yes | AlmaLinux 8 amd64 ( 20250225 _0013) | x86_64 | VIRTUAL-MACHINE | 906.01MiB | Feb 25, 2025 at 12:13am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 ( 3 more) | 289e62f91098 | yes | AlmaLinux 8 amd64 ( 20250225 _0013) | x86_64 | CONTAINER | 129.01MiB | Feb 25, 2025 at 12:13am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 /arm64 ( 1 more) | 457dc8ca19b1 | yes | AlmaLinux 8 arm64 ( 20250224 _0033) | aarch64 | CONTAINER | 125.25MiB | Feb 24, 2025 at 12:33am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+---------"},{"title":"Incus - ArchWiki","url":"https://wiki.archlinux.org/title/Incus","text":"- [Home](https://archlinux.org/)\n- [Packages](https://archlinux.org/packages/)\n- [Forums](https://bbs.archlinux.org/)\n- [Wiki](https://wiki.archlinux.org/)\n- [GitLab](https://gitlab.archlinux.org/archlinux)\n- [Security](https://security.archlinux.org/)\n- [AUR](https://aur.archlinux.org/)\n- [Download](https://archlinux.org/download/)\n\n [Jump to content](https://wiki.archlinux.org/title/Incus#bodyContent)\n\nFrom ArchWiki\n\nRelated articles\n\n- [Linux Containers](https://wiki.archlinux.org/title/Linux_Containers)\n- [LXD](https://wiki.archlinux.org/title/LXD)\n\n**[Incus](https://linuxcontainers.org/incus/introduction/)** is a manager/hypervisor for containers (via [LXC](https://wiki.archlinux.org/title/LXC)) and virtual-machines (via [QEMU](https://wiki.archlinux.org/title/QEMU)).\n\nIt is a fork of [LXD](https://ubuntu.com/lxd) by the original maintainers. Documentation from the [LXD](https://wiki.archlinux.org/title/LXD) wiki page is still largely relevant and encouraged reading.\n\n## Installation\n\n[Install](https://wiki.archlinux.org/title/Install) the [incus](https://archlinux.org/packages/?name=incus) package, then [enable](https://wiki.archlinux.org/title/Enable) the `incus.socket`.\n\nAlternatively, you can [enable/start](https://wiki.archlinux.org/title/Enable/start) the `incus.service` directly, in case you want instances to autostart for example.\n\nTo delegate container creation to users, [enable/start](https://wiki.archlinux.org/title/Enable/start) the `incus-user.socket` unit. See [#Accessing Incus as an unprivileged user](https://wiki.archlinux.org/title/Incus#Accessing_Incus_as_an_unprivileged_user) for group delegation.\n\n### Migrating from LXD\n\nIf you wish to migrate from an existing LXD installation, you should do so at this point, as the migration tool will only run against an empty target Incus server.\n\nAfter verifying that both the `lxc info` and `incus info` commands are running correctly, read the upstream [documentation](https://linuxcontainers.org/incus/docs/main/howto/server_migrate_lxd/) about the process, and afterwards run the migration tool:\n\n```\n# lxd-to-incus\n\n```\n\n## Configuration\n\n### Unprivileged containers\n\nIncus launches [unprivileged containers by default](https://linuxcontainers.org/incus/docs/main/explanation/security/#container-security) (see [Linux Containers#Privileged or unprivileged containers](https://wiki.archlinux.org/title/Linux_Containers#Privileged_or_unprivileged_containers) for an explanation of the difference).\n\nFor this to work, you need to setup an appropriate range of sub{u,g}ids for the **root** user [\\[1\\]](https://linuxcontainers.org/incus/docs/main/installing/#machine-setup): unlike _e.g._ [podman](https://wiki.archlinux.org/title/Podman), Incus uses a daemon that needs to run as root. [\\[2\\]](https://discuss.linuxcontainers.org/t/incus-no-uid-gid-allocation-configured/19002/13)\n\nVerify the content of both `/etc/subuid` and `/etc/subgid`, and if needed add a [contiguous range of at least 10M UID/GID](h"},{"title":"Ubuntu Manpage: incus-storage-volume-list - List storage volumes","url":"https://manpages.ubuntu.com/manpages/questing/man1/incus.storage.volume.list.1.html","text":"\n \nProvided by: incus-client_6.0.4-2_amd64 NAME incus-storage-volume-list - List storage volumes\n SYNOPSIS incus storage volume list [:] [...] [flags] \n DESCRIPTION Description:\nList storage volumes\nThe -c option takes a (optionally comma-separated) list of arguments\nthat control which image attributes to output when displaying in table\nor csv format.\nColumn shorthand chars:\nc - Content type (filesystem or block)\nd - Description\ne - Project name\nL - Location of the instance (e.g. its cluster member)\nn - Name\nt - Type of volume (custom, image, container or virtual-machine)\nu - Number of references (used by)\nU - Current disk usage\n OPTIONS --all-projects [=false] All projects``\n -c, --columns =\"etndcuL\" Columns``\n -f, --format =\"table\" Format (csv|json|table|yaml|compact), use suffix \",noheader\" to disable headers\nand \",header\" to enable it if missing, e.g. csv,header``\n OPTIONS INHERITED FROM PARENT COMMANDS --debug [=false] Show all debug messages\n --force-local [=false] Force using the local unix socket\n -h, --help [=false] Print help\n --project =\"\" Override the source project``\n -q, --quiet [=false] Don't show progress information\n --sub-commands [=false] Use with help or --help to view sub-commands\n -v, --verbose [=false] Show all information messages\n --version [=false] Print version number\n SEE ALSO incus-storage-volume (1) \n HISTORY 27-Apr-2025 Auto generated by spf13/cobra\n \n"},{"title":"incus storage volume list","url":"https://linuxcontainers.org/incus/docs/main/reference/manpages/incus/storage/volume/list/","text":"\n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n List storage volumes \n \n Synopsis ¶ \n \n Description: \n \n List storage volumes \n A single keyword like “vol” which will list any storage volume with a name starting by “vol”. \n A regular expression on the storage volume name. (e.g. .*vol.*01$). \n A key/value pair where the key is a storage volume field name. Multiple values must be delimited by ‘,’. \n Examples: \n \n - “type=custom” will list all custom storage volumes \n - “type=custom content_type=block” will list all custom block storage volumes \n \n == Columns == \n The -c option takes a (optionally comma-separated) list of arguments \n that control which image attributes to output when displaying in table \n or csv format. \n Column shorthand chars: \n \n c - Content type (filesystem or block) \n d - Description \n e - Project name \n L - Location of the instance (e.g. its cluster member) \n n - Name \n t - Type of volume (custom, image, container or virtual-machine) \n u - Number of references (used by) \n U - Current disk usage \n \n \n \n incus storage volume list [ &lt; remote &gt;:] &lt; pool &gt; [ &lt; filter &gt;...] [ flags] \n \n \n \n Options ¶ \n -- all - projects All projects \n - c, -- columns Columns ( default \"etndcuL\") \n - f, -- format Format ( csv | json | table | yaml | compact), use suffix \",noheader\" to disable headers and \",header\" to enable it if missing, e. g. csv, header ( default \"table\") \n \n \n \n Options inherited from parent commands ¶ \n -- debug Show all debug messages \n -- force - local Force using the local unix socket \n - h, -- help Print help \n -- project Override the source project \n - q, -- quiet Don 't show progress information \n -- sub - commands Use with help or -- help to view sub - commands \n - v, -- verbose Show all information messages \n -- version Print version number \n \n \n \n SEE ALSO ¶ \n \n incus storage volume - Manage storage volumes \n \n \n \n"},{"title":"incus list","url":"https://linuxcontainers.org/incus/docs/main/reference/manpages/incus/list/","text":"\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n List instances \n \n Synopsis ¶ \n \n Description: \n \n List instances \n Default column layout: ns46tS \n Fast column layout: nsacPt \n A single keyword like “web” which will list any instance with a name starting by “web”. \n A regular expression on the instance name. (e.g. .*web.*01$). \n A key/value pair referring to a configuration item. For those, the \n namespace can be abbreviated to the smallest unambiguous identifier. \n A key/value pair where the key is a shorthand. Multiple values must be delimited by ‘,’. Available shorthands: \n \n - type={instance type} \n - status={instance current lifecycle status} \n - architecture={instance architecture} \n - location={location name} \n - ipv4={ip or CIDR} \n - ipv6={ip or CIDR} \n \n Examples: \n \n - “user.blah=abc” will list all instances with the “blah” user property set to “abc”. \n - “u.blah=abc” will do the same \n - “security.privileged=true” will list all privileged instances \n - “s.privileged=true” will do the same \n - “type=container” will list all container instances \n - “type=container status=running” will list all running container instances \n \n A regular expression matching a configuration item or its value. (e.g. volatile.eth0.hwaddr=10:66:6a:.*). \n When multiple filters are passed, they are added one on top of the other, \n selecting instances which satisfy them all. \n == Columns == \n The -c option takes a comma separated list of arguments that control \n which instance attributes to output when displaying in table or csv \n format. \n Column arguments are either pre-defined shorthand chars (see below), \n or (extended) config keys. \n Commas between consecutive shorthand chars are optional. \n Pre-defined column shorthand chars: \n \n 4 - IPv4 address \n 6 - IPv6 address \n a - Architecture \n b - Storage pool \n c - Creation date \n d - Description \n D - disk usage \n e - Project name \n l - Last used date \n m - Memory usage \n M - Memory usage (%) \n n - Name \n N - Number of Processes \n p - PID of the instance’s init process \n P - Profiles \n s - State \n S - Number of snapshots \n t - Type (persistent or ephemeral) \n u - CPU usage (in seconds) \n U - Started date \n L - Location of the instance (e.g. its cluster member) \n f - Base Image Fingerprint (short) \n F - Base Image Fingerprint (long) \n \n Custom columns are defined with “[config:|devices:]key[:name][:maxWidth]”: \n \n KEY: The (extended) config or devices key to display. If [config:|devices:] is omitted then it defaults to config key. \n NAME: Name to display in the column header. \n Defaults to the key if not specified or empty. \n MAXWIDTH: Max width of the column (longer results are truncated). \n Defaults to -1 (unlimited). Use 0 to limit to the column header size. \n \n \n \n incus list [ &lt; remote &gt;:] [ &lt; filter &gt;...] [ flags] \n \n \n \n Examples ¶ \n incus list - c nFs46, volatile. eth0. hwaddr: MAC, config: image. os, devices: eth0. parent: ETHP \n Show instances using the \"NAME\", \"BASE "}]}


Debug Output:

```json
{
  "results": [
    {
      "title": "How to find Virtual Machine (VM) image with Incus / LXD",
      "url": "https://www.cyberciti.biz/faq/lxd-incus-finding-virtual-machine-vm-linux-command/",
      "text": "\n B oth Incus and LXD, are the powerful system container and virtual machine manager, simplifies many aspects of virtualization. But how do you find the right VM (virtual machine) only image to get you started? Incus and LXD does provides a robust search functionality that makes discovering and selecting images easy for Linux developers and sysadmins. \n \n Finding Your Virtual Machine (VM) image using list option \n \n The main command is to all list images. \n There are two types of images: a “container” and a “virtual machine.” \n To list container only images type: \n To list VM ( virtual machine) only images type: \n \n How to find Virtual Machine (VM) image with (LXD) \n The commands are same but the admin utility name is lxc command. In other words, type the following two commands to list different types of images: \n \n Show all images run: \n See all container images type: \n LXD see all virtual machine (VM) images run: \n \n \n \n Tutorial details \n Difficulty level Easy Root privileges No Requirements Linux terminal Category LXD OS compatibility AlmaLinux • Alpine • Amazon Linux • Arch • CentOS • Debian • Fedora • Linux • Mint • Mint • openSUSE • Pop!_OS • RHEL • Rocky • Slackware • Stream • SUSE • Ubuntu Est. reading time 3 minutes \n How to list only container images with operating system name \n When you type the following command, you get all images regardless of their architecture or operating system (OS) \n $ incus image list images: \n # LXD (LXC) # \n $ lxc image list images: \nOutputs: \n +------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| ALIAS | FINGERPRINT | PUBLIC | DESCRIPTION | ARCHITECTURE | TYPE | SIZE | UPLOAD DATE |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 ( 3 more) | 0c4bc559d302 | yes | AlmaLinux 8 amd64 ( 20250225 _0013) | x86_64 | VIRTUAL-MACHINE | 906.01MiB | Feb 25, 2025 at 12:13am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 ( 3 more) | 289e62f91098 | yes | AlmaLinux 8 amd64 ( 20250225 _0013) | x86_64 | CONTAINER | 129.01MiB | Feb 25, 2025 at 12:13am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+-------------------------------+\n| almalinux/ 8 /arm64 ( 1 more) | 457dc8ca19b1 | yes | AlmaLinux 8 arm64 ( 20250224 _0033) | aarch64 | CONTAINER | 125.25MiB | Feb 24, 2025 at 12:33am ( UTC) |\n+------------------------------------------+--------------+--------+-------------------------------------------+--------------+-----------------+------------+---------"
    },
    {
      "title": "Incus - ArchWiki",
      "url": "https://wiki.archlinux.org/title/Incus",
      "text": "- [Home](https://archlinux.org/)\n- [Packages](https://archlinux.org/packages/)\n- [Forums](https://bbs.archlinux.org/)\n- [Wiki](https://wiki.archlinux.org/)\n- [GitLab](https://gitlab.archlinux.org/archlinux)\n- [Security](https://security.archlinux.org/)\n- [AUR](https://aur.archlinux.org/)\n- [Download](https://archlinux.org/download/)\n\n [Jump to content](https://wiki.archlinux.org/title/Incus#bodyContent)\n\nFrom ArchWiki\n\nRelated articles\n\n- [Linux Containers](https://wiki.archlinux.org/title/Linux_Containers)\n- [LXD](https://wiki.archlinux.org/title/LXD)\n\n**[Incus](https://linuxcontainers.org/incus/introduction/)** is a manager/hypervisor for containers (via [LXC](https://wiki.archlinux.org/title/LXC)) and virtual-machines (via [QEMU](https://wiki.archlinux.org/title/QEMU)).\n\nIt is a fork of [LXD](https://ubuntu.com/lxd) by the original maintainers. Documentation from the [LXD](https://wiki.archlinux.org/title/LXD) wiki page is still largely relevant and encouraged reading.\n\n## Installation\n\n[Install](https://wiki.archlinux.org/title/Install) the [incus](https://archlinux.org/packages/?name=incus) package, then [enable](https://wiki.archlinux.org/title/Enable) the `incus.socket`.\n\nAlternatively, you can [enable/start](https://wiki.archlinux.org/title/Enable/start) the `incus.service` directly, in case you want instances to autostart for example.\n\nTo delegate container creation to users, [enable/start](https://wiki.archlinux.org/title/Enable/start) the `incus-user.socket` unit. See [#Accessing Incus as an unprivileged user](https://wiki.archlinux.org/title/Incus#Accessing_Incus_as_an_unprivileged_user) for group delegation.\n\n### Migrating from LXD\n\nIf you wish to migrate from an existing LXD installation, you should do so at this point, as the migration tool will only run against an empty target Incus server.\n\nAfter verifying that both the `lxc info` and `incus info` commands are running correctly, read the upstream [documentation](https://linuxcontainers.org/incus/docs/main/howto/server_migrate_lxd/) about the process, and afterwards run the migration tool:\n\n```\n# lxd-to-incus\n\n```\n\n## Configuration\n\n### Unprivileged containers\n\nIncus launches [unprivileged containers by default](https://linuxcontainers.org/incus/docs/main/explanation/security/#container-security) (see [Linux Containers#Privileged or unprivileged containers](https://wiki.archlinux.org/title/Linux_Containers#Privileged_or_unprivileged_containers) for an explanation of the difference).\n\nFor this to work, you need to setup an appropriate range of sub{u,g}ids for the **root** user [\\[1\\]](https://linuxcontainers.org/incus/docs/main/installing/#machine-setup): unlike _e.g._ [podman](https://wiki.archlinux.org/title/Podman), Incus uses a daemon that needs to run as root. [\\[2\\]](https://discuss.linuxcontainers.org/t/incus-no-uid-gid-allocation-configured/19002/13)\n\nVerify the content of both `/etc/subuid` and `/etc/subgid`, and if needed add a [contiguous range of at least 10M UID/GID](h"
    },
    {
      "title": "Ubuntu Manpage: incus-storage-volume-list - List storage volumes",
      "url": "https://manpages.ubuntu.com/manpages/questing/man1/incus.storage.volume.list.1.html",
      "text": "\n \nProvided by: incus-client_6.0.4-2_amd64 NAME incus-storage-volume-list - List storage volumes\n SYNOPSIS incus storage volume list [:] [...] [flags] \n DESCRIPTION Description:\nList storage volumes\nThe -c option takes a (optionally comma-separated) list of arguments\nthat control which image attributes to output when displaying in table\nor csv format.\nColumn shorthand chars:\nc - Content type (filesystem or block)\nd - Description\ne - Project name\nL - Location of the instance (e.g. its cluster member)\nn - Name\nt - Type of volume (custom, image, container or virtual-machine)\nu - Number of references (used by)\nU - Current disk usage\n OPTIONS --all-projects [=false] All projects``\n -c, --columns =\"etndcuL\" Columns``\n -f, --format =\"table\" Format (csv|json|table|yaml|compact), use suffix \",noheader\" to disable headers\nand \",header\" to enable it if missing, e.g. csv,header``\n OPTIONS INHERITED FROM PARENT COMMANDS --debug [=false] Show all debug messages\n --force-local [=false] Force using the local unix socket\n -h, --help [=false] Print help\n --project =\"\" Override the source project``\n -q, --quiet [=false] Don't show progress information\n --sub-commands [=false] Use with help or --help to view sub-commands\n -v, --verbose [=false] Show all information messages\n --version [=false] Print version number\n SEE ALSO incus-storage-volume (1) \n HISTORY 27-Apr-2025 Auto generated by spf13/cobra\n \n"
    },
    {
      "title": "incus storage volume list",
      "url": "https://linuxcontainers.org/incus/docs/main/reference/manpages/incus/storage/volume/list/",
      "text": "\n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n List storage volumes \n \n Synopsis ¶ \n \n Description: \n \n List storage volumes \n A single keyword like “vol” which will list any storage volume with a name starting by “vol”. \n A regular expression on the storage volume name. (e.g. .*vol.*01$). \n A key/value pair where the key is a storage volume field name. Multiple values must be delimited by ‘,’. \n Examples: \n \n - “type=custom” will list all custom storage volumes \n - “type=custom content_type=block” will list all custom block storage volumes \n \n == Columns == \n The -c option takes a (optionally comma-separated) list of arguments \n that control which image attributes to output when displaying in table \n or csv format. \n Column shorthand chars: \n \n c - Content type (filesystem or block) \n d - Description \n e - Project name \n L - Location of the instance (e.g. its cluster member) \n n - Name \n t - Type of volume (custom, image, container or virtual-machine) \n u - Number of references (used by) \n U - Current disk usage \n \n \n \n incus storage volume list [ &lt; remote &gt;:] &lt; pool &gt; [ &lt; filter &gt;...] [ flags] \n \n \n \n Options ¶ \n -- all - projects All projects \n - c, -- columns Columns ( default \"etndcuL\") \n - f, -- format Format ( csv | json | table | yaml | compact), use suffix \",noheader\" to disable headers and \",header\" to enable it if missing, e. g. csv, header ( default \"table\") \n \n \n \n Options inherited from parent commands ¶ \n -- debug Show all debug messages \n -- force - local Force using the local unix socket \n - h, -- help Print help \n -- project Override the source project \n - q, -- quiet Don 't show progress information \n -- sub - commands Use with help or -- help to view sub - commands \n - v, -- verbose Show all information messages \n -- version Print version number \n \n \n \n SEE ALSO ¶ \n \n incus storage volume - Manage storage volumes \n \n \n \n"
    },
    {
      "title": "incus list",
      "url": "https://linuxcontainers.org/incus/docs/main/reference/manpages/incus/list/",
      "text": "\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n List instances \n \n Synopsis ¶ \n \n Description: \n \n List instances \n Default column layout: ns46tS \n Fast column layout: nsacPt \n A single keyword like “web” which will list any instance with a name starting by “web”. \n A regular expression on the instance name. (e.g. .*web.*01$). \n A key/value pair referring to a configuration item. For those, the \n namespace can be abbreviated to the smallest unambiguous identifier. \n A key/value pair where the key is a shorthand. Multiple values must be delimited by ‘,’. Available shorthands: \n \n - type={instance type} \n - status={instance current lifecycle status} \n - architecture={instance architecture} \n - location={location name} \n - ipv4={ip or CIDR} \n - ipv6={ip or CIDR} \n \n Examples: \n \n - “user.blah=abc” will list all instances with the “blah” user property set to “abc”. \n - “u.blah=abc” will do the same \n - “security.privileged=true” will list all privileged instances \n - “s.privileged=true” will do the same \n - “type=container” will list all container instances \n - “type=container status=running” will list all running container instances \n \n A regular expression matching a configuration item or its value. (e.g. volatile.eth0.hwaddr=10:66:6a:.*). \n When multiple filters are passed, they are added one on top of the other, \n selecting instances which satisfy them all. \n == Columns == \n The -c option takes a comma separated list of arguments that control \n which instance attributes to output when displaying in table or csv \n format. \n Column arguments are either pre-defined shorthand chars (see below), \n or (extended) config keys. \n Commas between consecutive shorthand chars are optional. \n Pre-defined column shorthand chars: \n \n 4 - IPv4 address \n 6 - IPv6 address \n a - Architecture \n b - Storage pool \n c - Creation date \n d - Description \n D - disk usage \n e - Project name \n l - Last used date \n m - Memory usage \n M - Memory usage (%) \n n - Name \n N - Number of Processes \n p - PID of the instance’s init process \n P - Profiles \n s - State \n S - Number of snapshots \n t - Type (persistent or ephemeral) \n u - CPU usage (in seconds) \n U - Started date \n L - Location of the instance (e.g. its cluster member) \n f - Base Image Fingerprint (short) \n F - Base Image Fingerprint (long) \n \n Custom columns are defined with “[config:|devices:]key[:name][:maxWidth]”: \n \n KEY: The (extended) config or devices key to display. If [config:|devices:] is omitted then it defaults to config key. \n NAME: Name to display in the column header. \n Defaults to the key if not specified or empty. \n MAXWIDTH: Max width of the column (longer results are truncated). \n Defaults to -1 (unlimited). Use 0 to limit to the column header size. \n \n \n \n incus list [ &lt; remote &gt;:] [ &lt; filter &gt;...] [ flags] \n \n \n \n Examples ¶ \n incus list - c nFs46, volatile. eth0. hwaddr: MAC, config: image. os, devices: eth0. parent: ETHP \n Show instances using the \"NAME\", \"BASE "
    }
  ]
}
```

## Agent

Now let's gather information on how to set up networking for a VM to install k3s:

**Use Tool: web_search (toolu_015L8m8tRE7xrBTzf4uL292N)**
```json
{
  "query": "incus VM network configuration for k3s"
}
```

**Tool Results: toolu_015L8m8tRE7xrBTzf4uL292N**

{"results":[{"title":"incus_network | Resources | lxc/incus | Terraform","url":"https://registry.terraform.io/providers/lxc/incus/latest/docs/resources/network","text":"\n \n \n \n Manages an Incus network. \n See Incus networking configuration reference \nfor all network details. \n Example Usage \n resource \"incus_network\" \"new_default\" { \n name = \"new_default\" \n config = { \n \"ipv4.address\" = \"10.150.19.1/24\" \n \"ipv4.nat\" = \"true\" \n \"ipv6.address\" = \"fd42:474b:622d:259d::1/64\" \n \"ipv6.nat\" = \"true\" \n} \n} \n resource \"incus_profile\" \"profile1\" { \n name = \"profile1\" \n device { \n name = \"eth0\" \n type = \"nic\" \n properties = { \n nictype = \"bridged\" \n parent = \"${incus_network.new_default.name}\" \n} \n} \n device { \n type = \"disk\" \n name = \"root\" \n properties = { \n pool = \"default\" \n path = \"/\" \n} \n} \n} \n resource \"incus_instance\" \"test1\" { \n name = \"test1\" \n image = \"ubuntu\" \n ephemeral = false \n profiles = [ \"${incus_profile.profile1.name}\"] \n} \n Multiple Network Example \n This example uses the \"default\" Incus network on eth0 (unspecified) and a\ncustom network on eth1 \n resource \"incus_network\" \"internal\" { \n name = \"internal\" \n config = { \n \"ipv4.address\" = \"192.168.255.1/24\" \n} \n} \n resource \"incus_profile\" \"profile1\" { \n name = \"profile1\" \n device { \n name = \"eth1\" \n type = \"nic\" \n properties = { \n nictype = \"bridged\" \n parent = \"${incus_network.internal.name}\" \n} \n} \n device { \n type = \"disk\" \n name = \"root\" \n properties = { \n pool = \"default\" \n path = \"/\" \n} \n} \n} \n resource \"incus_instance\" \"test1\" { \n name = \"test1\" \n image = \"ubuntu\" \n ephemeral = false \n profiles = [ \"default\", \"${incus_profile.profile1.name}\"] \n provisioner \"local-exec\" { \n command = \"lxc exec local:${self.name} dhclient eth1\" \n} \n} \n Tunnel Example \n Tunnel \"server\": \n resource \"incus_network\" \"vxtun\" { \n name = \"vxtun\" \n config = { \n \"tunnel.vxtun.protocol\" = \"vxlan\" \n \"tunnel.vxtun.id\" = 9999 \n \"tunnel.vxtun.local\" = \"10.1.1.1\" \n \"tunnel.vxtun.remote\" = \"10.255.1.1\" \n \"ipv4.address\" = \"192.168.255.1/24\" \n \"ipv6.address\" = \"none\" \n} \n} \n Tunnel \"client\": \n resource \"incus_network\" \"vxtun\" { \n name = \"vxtun\" \n config = { \n \"tunnel.vxtun.protocol\" = \"vxlan\" \n \"tunnel.vxtun.id\" = 9999 \n \"tunnel.vxtun.local\" = \"10.255.1.1\" \n \"tunnel.vxtun.remote\" = \"10.1.1.1\" \n \"ipv4.address\" = \"none\" \n \"ipv6.address\" = \"none\" \n} \n} \n Note how the local and remote addresses are swapped between the two.\nAlso note how the client does not provide an IP address range. \n Cluster Example \n In order to create a network in a cluster, you first have to\ndefine the network on each node in the cluster. Then you can create\nthe actual network: \n resource \"incus_network\" \"my_network_node1\" { \n name = \"my_network\" \n target = \"node1\" \n} \n resource \"incus_network\" \"my_network_node2\" { \n name = \"my_network\" \n target = \"node2\" \n} \n resource \"incus_network\" \"my_network\" { \n depends_on = [ \n \"incus_network.my_network_node1\",\n \"incus_network.my_network_node2\",\n] \n name = \"my_network\" \n config = { \n \"ipv4.address\" = \"10.150.19.1/24\" \n \"ipv4.nat\" = \"true\" \n \"ipv6.address\" = \"fd42:474b:622d:259d::1/64\" \n \"ipv6.nat\" = \"true\" \n} \n} \n Please see the Incus Clustering documentation \nfor more detail"},{"title":"How to create a network","url":"https://linuxcontainers.org/incus/docs/main/howto/network_create/","text":"\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n To create a managed network, use the incus network command and its subcommands.\nAppend --help to any command to see more information about its usage and available flags. \n \n Network types ¶ \n The following network types are available: \n \n \n Create a network ¶ \n Use the following command to create a network: \n incus network create &lt;name&gt; --type = &lt;network_type&gt; [ configuration_options...] \n \n See Network types for a list of available network types and links to their configuration options. \n If you do not specify a --type argument, the default type of bridge is used. \n \n Create a network in a cluster ¶ \n If you are running an Incus cluster and want to create a network, you must create the network for each cluster member separately.\nThe reason for this is that the network configuration, for example, the name of the parent network interface, might be different between cluster members. \n Therefore, you must first create a pending network on each member with the --target=&lt;cluster_member&gt; flag and the appropriate configuration for the member.\nMake sure to use the same network name for all members.\nThen create the network without specifying the --target flag to actually set it up. \n For example, the following series of commands sets up a physical network with the name UPLINK on three cluster members: \n \n ~$ incus network create UPLINK --type=physical parent=br0 --target=vm01 \n Network UPLINK pending on member vm01 ~$ incus network create UPLINK --type=physical parent=br0 --target=vm02 \n Network UPLINK pending on member vm02 ~$ incus network create UPLINK --type=physical parent=br0 --target=vm03 \n Network UPLINK pending on member vm03 ~$ incus network create UPLINK --type=physical \n Network UPLINK created \n Also see How to configure networks for a cluster. \n \n \n \n Attach a network to an instance ¶ \n After creating a managed network, you can attach it to an instance as a NIC device. \n To do so, use the following command: \n incus network attach &lt;network_name&gt; &lt;instance_name&gt; [&lt;device_name&gt;] [&lt;interface_name&gt;]\n \n The device name and the interface name are optional, but we recommend specifying at least the device name.\nIf not specified, Incus uses the network name as the device name, which might be confusing and cause problems.\nFor example, Incus images perform IP auto-configuration on the eth0 interface, which does not work if the interface is called differently. \n For example, to attach the network my-network to the instance my-instance as eth0 device, enter the following command: \n incus network attach my-network my-instance eth0\n \n \n Attach the network as a device ¶ \n The incus network attach command is a shortcut for adding a NIC device to an instance.\nAlternatively, you can add a NIC device based on the network configuration in the usual way: \n incus config device add &lt;instance_name&gt; &lt;device_name&gt; nic network=&lt;network_name&gt;"},{"title":"Networking","url":"https://linuxcontainers.org/incus/docs/main/networks/","text":"\n \n Toggle site navigation sidebar \n \n \n \n Incus documentation \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n Back to top \n \n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n \n \n About networking \n Create and configure a network \n Configure a network \n Configure network ACLs \n Configure network address sets \n Configure network forwards \n Configure network integrations \n Configure network zones \n Configure Incus as BGP server \n Display Incus IPAM information \n Bridge network \n OVN network \n External networks \n Increase bandwidth \n \n \n \n \n \n"},{"title":"How to configure a network","url":"https://linuxcontainers.org/incus/docs/main/howto/network_configure/","text":"\n \n Back to top \n \n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To configure an existing network, use either the incus network set and incus network unset commands (to configure single settings) or the incus network edit command (to edit the full configuration).\nTo configure settings for specific cluster members, add the --target flag. \n For example, the following command configures a DNS server for a physical network: \n incus network set UPLINK dns.nameservers = 8.8.8.8\n \n The available configuration options differ depending on the network type.\nSee Network types for links to the configuration options for each network type. \n There are separate commands to configure advanced networking features.\nSee the following documentation: \n \n How to configure network ACLs \n How to configure network forwards \n How to configure network integrations \n How to configure network load balancers \n How to configure network zones \n How to create peer routing relationships (OVN only) \n \n \n \n"},{"title":"How to configure networks for a cluster","url":"https://linuxcontainers.org/incus/docs/main/howto/cluster_config_networks/","text":"\n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n All members of a cluster must have identical networks defined.\nThe only configuration keys that may differ between networks on different members are bridge.external_interfaces, parent, bgp.ipv4.nexthop and bgp.ipv6.nexthop.\nSee Member configuration for more information. \n Creating additional networks is a two-step process: \n \n Define and configure the new network across all cluster members.\nFor example, for a cluster that has three members: \n incus network create --target server1 my-network\nincus network create --target server2 my-network\nincus network create --target server3 my-network\n \n \n Note \n You can pass only the member-specific configuration keys bridge.external_interfaces, parent, bgp.ipv4.nexthop and bgp.ipv6.nexthop.\nPassing other configuration keys results in an error. \n \n These commands define the network, but they don’t create it.\nIf you run incus network list, you can see that the network is marked as “pending”. \n \n Run the following command to instantiate the network on all cluster members: \n incus network create my-network\n \n \n Note \n You can add configuration keys that are not member-specific to this command. \n \n If you missed a cluster member when defining the network, or if a cluster member is down, you get an error. \n \n \n Also see Create a network in a cluster. \n \n Separate REST API and clustering networks ¶ \n You can configure different networks for the REST API endpoint of your clients and for internal traffic between the members of your cluster.\nThis separation can be useful, for example, to use a virtual address for your REST API, with DNS round robin. \n To do so, you must specify different addresses for cluster.https_address (the address for internal cluster traffic) and core.https_address (the address for the REST API): \n \n Create your cluster as usual, and make sure to use the address that you want to use for internal cluster traffic as the cluster address.\nThis address is set as the cluster.https_address configuration. \n After joining your members, set the core.https_address configuration to the address for the REST API.\nFor example: \n incus config set core.https_address 0.0.0.0:8443\n \n \n Note \n core.https_address is specific to the cluster member, so you can use different addresses on different members.\nYou can also use a wildcard address to make the member listen on multiple interfaces. \n \n \n \n \n \n"}]}


Debug Output:

```json
{
  "results": [
    {
      "title": "incus_network | Resources | lxc/incus | Terraform",
      "url": "https://registry.terraform.io/providers/lxc/incus/latest/docs/resources/network",
      "text": "\n \n \n \n Manages an Incus network. \n See Incus networking configuration reference \nfor all network details. \n Example Usage \n resource \"incus_network\" \"new_default\" { \n name = \"new_default\" \n config = { \n \"ipv4.address\" = \"10.150.19.1/24\" \n \"ipv4.nat\" = \"true\" \n \"ipv6.address\" = \"fd42:474b:622d:259d::1/64\" \n \"ipv6.nat\" = \"true\" \n} \n} \n resource \"incus_profile\" \"profile1\" { \n name = \"profile1\" \n device { \n name = \"eth0\" \n type = \"nic\" \n properties = { \n nictype = \"bridged\" \n parent = \"${incus_network.new_default.name}\" \n} \n} \n device { \n type = \"disk\" \n name = \"root\" \n properties = { \n pool = \"default\" \n path = \"/\" \n} \n} \n} \n resource \"incus_instance\" \"test1\" { \n name = \"test1\" \n image = \"ubuntu\" \n ephemeral = false \n profiles = [ \"${incus_profile.profile1.name}\"] \n} \n Multiple Network Example \n This example uses the \"default\" Incus network on eth0 (unspecified) and a\ncustom network on eth1 \n resource \"incus_network\" \"internal\" { \n name = \"internal\" \n config = { \n \"ipv4.address\" = \"192.168.255.1/24\" \n} \n} \n resource \"incus_profile\" \"profile1\" { \n name = \"profile1\" \n device { \n name = \"eth1\" \n type = \"nic\" \n properties = { \n nictype = \"bridged\" \n parent = \"${incus_network.internal.name}\" \n} \n} \n device { \n type = \"disk\" \n name = \"root\" \n properties = { \n pool = \"default\" \n path = \"/\" \n} \n} \n} \n resource \"incus_instance\" \"test1\" { \n name = \"test1\" \n image = \"ubuntu\" \n ephemeral = false \n profiles = [ \"default\", \"${incus_profile.profile1.name}\"] \n provisioner \"local-exec\" { \n command = \"lxc exec local:${self.name} dhclient eth1\" \n} \n} \n Tunnel Example \n Tunnel \"server\": \n resource \"incus_network\" \"vxtun\" { \n name = \"vxtun\" \n config = { \n \"tunnel.vxtun.protocol\" = \"vxlan\" \n \"tunnel.vxtun.id\" = 9999 \n \"tunnel.vxtun.local\" = \"10.1.1.1\" \n \"tunnel.vxtun.remote\" = \"10.255.1.1\" \n \"ipv4.address\" = \"192.168.255.1/24\" \n \"ipv6.address\" = \"none\" \n} \n} \n Tunnel \"client\": \n resource \"incus_network\" \"vxtun\" { \n name = \"vxtun\" \n config = { \n \"tunnel.vxtun.protocol\" = \"vxlan\" \n \"tunnel.vxtun.id\" = 9999 \n \"tunnel.vxtun.local\" = \"10.255.1.1\" \n \"tunnel.vxtun.remote\" = \"10.1.1.1\" \n \"ipv4.address\" = \"none\" \n \"ipv6.address\" = \"none\" \n} \n} \n Note how the local and remote addresses are swapped between the two.\nAlso note how the client does not provide an IP address range. \n Cluster Example \n In order to create a network in a cluster, you first have to\ndefine the network on each node in the cluster. Then you can create\nthe actual network: \n resource \"incus_network\" \"my_network_node1\" { \n name = \"my_network\" \n target = \"node1\" \n} \n resource \"incus_network\" \"my_network_node2\" { \n name = \"my_network\" \n target = \"node2\" \n} \n resource \"incus_network\" \"my_network\" { \n depends_on = [ \n \"incus_network.my_network_node1\",\n \"incus_network.my_network_node2\",\n] \n name = \"my_network\" \n config = { \n \"ipv4.address\" = \"10.150.19.1/24\" \n \"ipv4.nat\" = \"true\" \n \"ipv6.address\" = \"fd42:474b:622d:259d::1/64\" \n \"ipv6.nat\" = \"true\" \n} \n} \n Please see the Incus Clustering documentation \nfor more detail"
    },
    {
      "title": "How to create a network",
      "url": "https://linuxcontainers.org/incus/docs/main/howto/network_create/",
      "text": "\n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n To create a managed network, use the incus network command and its subcommands.\nAppend --help to any command to see more information about its usage and available flags. \n \n Network types ¶ \n The following network types are available: \n \n \n Create a network ¶ \n Use the following command to create a network: \n incus network create &lt;name&gt; --type = &lt;network_type&gt; [ configuration_options...] \n \n See Network types for a list of available network types and links to their configuration options. \n If you do not specify a --type argument, the default type of bridge is used. \n \n Create a network in a cluster ¶ \n If you are running an Incus cluster and want to create a network, you must create the network for each cluster member separately.\nThe reason for this is that the network configuration, for example, the name of the parent network interface, might be different between cluster members. \n Therefore, you must first create a pending network on each member with the --target=&lt;cluster_member&gt; flag and the appropriate configuration for the member.\nMake sure to use the same network name for all members.\nThen create the network without specifying the --target flag to actually set it up. \n For example, the following series of commands sets up a physical network with the name UPLINK on three cluster members: \n \n ~$ incus network create UPLINK --type=physical parent=br0 --target=vm01 \n Network UPLINK pending on member vm01 ~$ incus network create UPLINK --type=physical parent=br0 --target=vm02 \n Network UPLINK pending on member vm02 ~$ incus network create UPLINK --type=physical parent=br0 --target=vm03 \n Network UPLINK pending on member vm03 ~$ incus network create UPLINK --type=physical \n Network UPLINK created \n Also see How to configure networks for a cluster. \n \n \n \n Attach a network to an instance ¶ \n After creating a managed network, you can attach it to an instance as a NIC device. \n To do so, use the following command: \n incus network attach &lt;network_name&gt; &lt;instance_name&gt; [&lt;device_name&gt;] [&lt;interface_name&gt;]\n \n The device name and the interface name are optional, but we recommend specifying at least the device name.\nIf not specified, Incus uses the network name as the device name, which might be confusing and cause problems.\nFor example, Incus images perform IP auto-configuration on the eth0 interface, which does not work if the interface is called differently. \n For example, to attach the network my-network to the instance my-instance as eth0 device, enter the following command: \n incus network attach my-network my-instance eth0\n \n \n Attach the network as a device ¶ \n The incus network attach command is a shortcut for adding a NIC device to an instance.\nAlternatively, you can add a NIC device based on the network configuration in the usual way: \n incus config device add &lt;instance_name&gt; &lt;device_name&gt; nic network=&lt;network_name&gt;"
    },
    {
      "title": "Networking",
      "url": "https://linuxcontainers.org/incus/docs/main/networks/",
      "text": "\n \n Toggle site navigation sidebar \n \n \n \n Incus documentation \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n Back to top \n \n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n \n \n About networking \n Create and configure a network \n Configure a network \n Configure network ACLs \n Configure network address sets \n Configure network forwards \n Configure network integrations \n Configure network zones \n Configure Incus as BGP server \n Display Incus IPAM information \n Bridge network \n OVN network \n External networks \n Increase bandwidth \n \n \n \n \n \n"
    },
    {
      "title": "How to configure a network",
      "url": "https://linuxcontainers.org/incus/docs/main/howto/network_configure/",
      "text": "\n \n Back to top \n \n \n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n \n To configure an existing network, use either the incus network set and incus network unset commands (to configure single settings) or the incus network edit command (to edit the full configuration).\nTo configure settings for specific cluster members, add the --target flag. \n For example, the following command configures a DNS server for a physical network: \n incus network set UPLINK dns.nameservers = 8.8.8.8\n \n The available configuration options differ depending on the network type.\nSee Network types for links to the configuration options for each network type. \n There are separate commands to configure advanced networking features.\nSee the following documentation: \n \n How to configure network ACLs \n How to configure network forwards \n How to configure network integrations \n How to configure network load balancers \n How to configure network zones \n How to create peer routing relationships (OVN only) \n \n \n \n"
    },
    {
      "title": "How to configure networks for a cluster",
      "url": "https://linuxcontainers.org/incus/docs/main/howto/cluster_config_networks/",
      "text": "\n \n \n View this page \n \n \n Toggle table of contents sidebar \n \n \n \n \n All members of a cluster must have identical networks defined.\nThe only configuration keys that may differ between networks on different members are bridge.external_interfaces, parent, bgp.ipv4.nexthop and bgp.ipv6.nexthop.\nSee Member configuration for more information. \n Creating additional networks is a two-step process: \n \n Define and configure the new network across all cluster members.\nFor example, for a cluster that has three members: \n incus network create --target server1 my-network\nincus network create --target server2 my-network\nincus network create --target server3 my-network\n \n \n Note \n You can pass only the member-specific configuration keys bridge.external_interfaces, parent, bgp.ipv4.nexthop and bgp.ipv6.nexthop.\nPassing other configuration keys results in an error. \n \n These commands define the network, but they don’t create it.\nIf you run incus network list, you can see that the network is marked as “pending”. \n \n Run the following command to instantiate the network on all cluster members: \n incus network create my-network\n \n \n Note \n You can add configuration keys that are not member-specific to this command. \n \n If you missed a cluster member when defining the network, or if a cluster member is down, you get an error. \n \n \n Also see Create a network in a cluster. \n \n Separate REST API and clustering networks ¶ \n You can configure different networks for the REST API endpoint of your clients and for internal traffic between the members of your cluster.\nThis separation can be useful, for example, to use a virtual address for your REST API, with DNS round robin. \n To do so, you must specify different addresses for cluster.https_address (the address for internal cluster traffic) and core.https_address (the address for the REST API): \n \n Create your cluster as usual, and make sure to use the address that you want to use for internal cluster traffic as the cluster address.\nThis address is set as the cluster.https_address configuration. \n After joining your members, set the core.https_address configuration to the address for the REST API.\nFor example: \n incus config set core.https_address 0.0.0.0:8443\n \n \n Note \n core.https_address is specific to the cluster member, so you can use different addresses on different members.\nYou can also use a wildcard address to make the member listen on multiple interfaces. \n \n \n \n \n \n"
    }
  ]
}
```

## Agent

Based on all the gathered information, I can now provide you with a detailed guide on how to create an AlmaLinux 9.6 VM with Incus for testing K3s.

## Creating an AlmaLinux 9.6 VM with Incus for K3s Testing

### Step 1: Check Available AlmaLinux Images

First, let's check what AlmaLinux images are available through Incus:

```bash
incus image list images: | grep almalinux
```

This command will list all available AlmaLinux images. If you want to specifically see only VM images, you can use:

```bash
incus image list images: --vm | grep almalinux
```

### Step 2: Launch an AlmaLinux 9 VM

Based on the information I gathered, AlmaLinux 9.6 images are available in Incus. You can create the VM with the following command:

```bash
incus launch images:almalinux/9 almalinux-k3s --vm
```

If you want to specify a larger disk size (recommended for testing K3s), you can use:

```bash
incus launch images:almalinux/9 almalinux-k3s --vm --device root,size=20GiB
```

### Step 3: Configure Networking

For K3s to work properly, you'll need proper networking. There are a few options:

#### Option 1: Use Bridge Networking (recommended)

If you want your VM to have its own IP address on your network:

```bash
incus network create br0 --type=bridge
incus network attach br0 almalinux-k3s eth0
```

Or, if you already have a bridge network set up:

```bash
incus config device add almalinux-k3s eth0 nic nictype=bridged parent=br0
```

#### Option 2: Use NAT Networking (simpler but less flexible)

This option uses Incus's default NAT network:

```bash
incus config device add almalinux-k3s eth0 nic nictype=bridged parent=lxdbr0
```

### Step 4: Access the VM

Once the VM is running, you can access it with:

```bash
incus console almalinux-k3s
```

Alternatively, you can use SSH if you've configured networking correctly. First, you need to get the IP address of your VM:

```bash
incus list almalinux-k3s
```

### Step 5: Configure the VM for K3s

Once inside the VM, update the system and install necessary packages:

```bash
dnf update -y
dnf install -y curl sudo
```

### Step 6: Disable firewalld and SELinux (often recommended for K3s testing)

```bash
systemctl disable --now firewalld
setenforce 0
```

To make SELinux changes permanent, edit `/etc/selinux/config` and set `SELINUX=permissive`.

### Step 7: Install K3s

You can now install K3s using the standard installation script:

```bash
curl -sfL https://get.k3s.io | sh -
```

If you want to install K3s without it starting as a service (for more customized testing):

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true sh -
```

### Step 8: Verify K3s Installation

After installation, you can verify K3s is working properly:

```bash
kubectl get nodes
```

## Additional Configuration Options

### Configuring Resource Limits

If you want to allocate specific resources to your VM:

```bash
incus config set almalinux-k3s limits.cpu 2
incus config set almalinux-k3s limits.memory 4GB
```

### Making the VM Autostart

If you want the VM to start automatically when your Incus service starts:

```bash
incus config set almalinux-k3s boot.autostart true
```

### Creating a Snapshot Before Installing K3s

It's a good practice to create a snapshot before installing K3s, so you can easily revert if needed:

```bash
incus snapshot almalinux-k3s pre-k3s
```

To restore the snapshot later:

```bash
incus restore almalinux-k3s pre-k3s
```

## Troubleshooting

If you encounter issues:

1. Check VM status: `incus info almalinux-k3s`
2. View VM logs: `incus console almalinux-k3s --show-log`
3. Check system journal inside the VM: `journalctl -xe`
4. For K3s-specific issues, check K3s logs: `journalctl -u k3s`

By following these steps, you should have a working AlmaLinux 9.6 VM with Incus that's ready for testing K3s. The VM will have its own IP address, making it easy to interact with the K3s cluster from your host machine.
